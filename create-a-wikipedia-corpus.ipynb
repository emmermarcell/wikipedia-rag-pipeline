{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/emmermarcell/create-a-wikipedia-corpus?scriptVersionId=159940263\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Create a Wikipedia corpus\n\nThe aim of this notebook is to preprocess an existing wikipedia dump and create a wiikpedia corpus can can be used for NLP tasks. Since the dataset is quite large I will make use of the memory mapping between the RAM and the filesystems storage done by the the [Hugging Face Datasets library][1]. Under the hood, it utilizes the Apache Arrow memory format and pyarrow library. Unfortunately the [`wikipedia`][8] dataset is not streamable so I stick to iterating through it.\n\n[1]: https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt\n[8]: https://huggingface.co/datasets/wikipedia","metadata":{}},{"cell_type":"code","source":"!pip install blingfire","metadata":{"execution":{"iopub.status.busy":"2024-01-22T00:42:47.759112Z","iopub.execute_input":"2024-01-22T00:42:47.759849Z","iopub.status.idle":"2024-01-22T00:43:08.364417Z","shell.execute_reply.started":"2024-01-22T00:42:47.759805Z","shell.execute_reply":"2024-01-22T00:43:08.362831Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting blingfire\n  Downloading blingfire-0.1.8-py3-none-any.whl (42.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: blingfire\nSuccessfully installed blingfire-0.1.8\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport glob\nimport gzip\nimport gc    # Garbage collector\nfrom tqdm.auto import tqdm\n\nfrom blingfire import text_to_sentences\nimport numpy as np\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-22T00:43:08.367519Z","iopub.execute_input":"2024-01-22T00:43:08.367999Z","iopub.status.idle":"2024-01-22T00:43:10.106021Z","shell.execute_reply.started":"2024-01-22T00:43:08.367956Z","shell.execute_reply":"2024-01-22T00:43:10.104768Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"The `all-MiniLM-L6-v2` model can handle a maximum sequence size of 256 words. As Wikipedia articles are often longer, I use the `text_to_sentences` function from`blingfire` to preprocess the data. This function takes a string (representing a Wikipedia article) and breaks up the article by sentences. The sentences are then saved into the `wikipedia_processed_*.txt` files locally line-by-line. After, I can simply utilize the Huggingface datasets library. ","metadata":{}},{"cell_type":"code","source":"def process_wikipedia_dataset(wiki_dataset, output_dir, articles_per_file=1_000_000):\n    file_count = 1\n    article_count = 0\n\n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Open the first output file for writing in compressed format\n    out_f = gzip.open(f'{output_dir}/wikipedia_processed_{file_count}.txt.gz', 'wt', encoding='utf-8')\n\n    # Iterate through each article in the dataset\n    for article in tqdm(wiki_dataset, desc='Processing Articles'):\n        # Process each line of the article into sentences\n        for line in article['text'].split('\\n'):\n            sentences = text_to_sentences(line)\n            out_f.write(sentences + '\\n')\n\n        article_count += 1\n        # Check if it's time to switch to a new file\n        if article_count >= articles_per_file:\n            out_f.close()\n            file_count += 1\n            article_count = 0\n            out_f = gzip.open(f'{output_dir}/wikipedia_processed_{file_count}.txt.gz', 'wt', encoding='utf-8')\n\n    # Close the last file\n    out_f.close()\n\n    # Pattern to match the compressed files\n    processed_file_paths = f\"{output_dir}/wikipedia_processed_*.txt.gz\"\n    # List all compressed files matching the pattern\n    processed_file_list = glob.glob(processed_file_paths)\n    print(f'The processed files are: {processed_file_list}')\n    \n    return processed_file_list","metadata":{"execution":{"iopub.status.busy":"2024-01-22T00:43:10.10736Z","iopub.execute_input":"2024-01-22T00:43:10.108056Z","iopub.status.idle":"2024-01-22T00:43:10.117184Z","shell.execute_reply.started":"2024-01-22T00:43:10.108013Z","shell.execute_reply":"2024-01-22T00:43:10.116315Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Load the Wikipedia dataset\nwiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split='train')\nprint(f'Length of the Wikipedia dataset is {len(wiki_dataset)} articles.')\n\n# Specify the working directory to save the processed files\noutput_dir = '/kaggle/working'\n# Process the Wikipedia dataset and return the path to the processed files\nprocessed_file_list = process_wikipedia_dataset(wiki_dataset, output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T00:43:10.119095Z","iopub.execute_input":"2024-01-22T00:43:10.119466Z","iopub.status.idle":"2024-01-22T02:54:13.497081Z","shell.execute_reply.started":"2024-01-22T00:43:10.119423Z","shell.execute_reply":"2024-01-22T02:54:13.495669Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/11.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8f27714639b4b7ca386e5f34632cbb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/7.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48e8d4c87f5f41ef8b96a1ad1cdcc274"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset wikipedia/20220301.en (download: 19.18 GiB, generated: 18.88 GiB, post-processed: Unknown size, total: 38.07 GiB) to /root/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/15.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09bb8e9ebd41450496ec22268b5ff721"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/20.3G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"472c229ac7fc449a89ed62e9d63ed7e6"}},"metadata":{}},{"name":"stdout","text":"Dataset wikipedia downloaded and prepared to /root/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559. Subsequent calls will reuse this data.\nLength of the Wikipedia dataset is 6458670 articles.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Articles:   0%|          | 0/6458670 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ada259fbbc4402c91316e43d454feac"}},"metadata":{}},{"name":"stdout","text":"The processed files are: ['/kaggle/working/wikipedia_processed_2.txt.gz', '/kaggle/working/wikipedia_processed_3.txt.gz', '/kaggle/working/wikipedia_processed_5.txt.gz', '/kaggle/working/wikipedia_processed_1.txt.gz', '/kaggle/working/wikipedia_processed_6.txt.gz', '/kaggle/working/wikipedia_processed_7.txt.gz', '/kaggle/working/wikipedia_processed_4.txt.gz']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### The resulting files can be saved in a Kaggle dataset.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}