{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/emmermarcell/create-a-wikipedia-corpus?scriptVersionId=163522301\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Create a Wikipedia corpus\n\nThe aim of this notebook is to preprocess an existing wikipedia dump and create a wiikpedia corpus can can be used for NLP tasks. Since the dataset is quite large I will make use of the memory mapping between the RAM and the filesystems storage done by the the [Hugging Face Datasets library][1]. Under the hood, it utilizes the Apache Arrow memory format and pyarrow library. Unfortunately the [`wikipedia`][2] dataset is not streamable so I stick to iterating through it.\n\nI used the following articles and notebooks as a starting point for implementing a RAG pipeline:\n\n* [Steven van de Graaf - Pre-processing a Wikipedia dump for NLP model training — a write-up][3]\n\n* [Chris Deotte - How To Train Open Book Model - Part 2][4]\n\n[1]: https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt\n[2]: https://huggingface.co/datasets/wikipedia\n[3]: https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67\n[4]: https://www.kaggle.com/code/cdeotte/how-to-train-open-book-model-part-2","metadata":{}},{"cell_type":"code","source":"!pip install blingfire","metadata":{"execution":{"iopub.status.busy":"2024-02-19T21:32:30.707818Z","iopub.execute_input":"2024-02-19T21:32:30.708594Z","iopub.status.idle":"2024-02-19T21:32:48.778983Z","shell.execute_reply.started":"2024-02-19T21:32:30.708528Z","shell.execute_reply":"2024-02-19T21:32:48.777608Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting blingfire\n  Downloading blingfire-0.1.8-py3-none-any.whl (42.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: blingfire\nSuccessfully installed blingfire-0.1.8\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport time\nimport re\nimport gzip\nimport gc    # Garbage collector\nfrom tqdm.auto import tqdm\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\n\nimport blingfire as bf\nimport numpy as np\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-02-19T21:32:48.782527Z","iopub.execute_input":"2024-02-19T21:32:48.783348Z","iopub.status.idle":"2024-02-19T21:32:50.622212Z","shell.execute_reply.started":"2024-02-19T21:32:48.783298Z","shell.execute_reply":"2024-02-19T21:32:50.621036Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Compile regular expressions only once at the beginning\ninfobox_pattern = re.compile(r'\\{\\{Infobox [^}]+\\}\\}', flags=re.DOTALL)\nsidebar_pattern = re.compile(r'\\{\\{Sidebar [^}]+\\}\\}', flags=re.DOTALL)\nlink_pattern = re.compile(r'\\[\\[([^|\\]]+\\|)?([^\\]]+)\\]\\]')\nreferences_pattern = re.compile(r'==\\s*(References|External links|See also|Notes)\\s*==.*', flags=re.DOTALL)\ncitation_needed_pattern = re.compile(r'\\{\\{citation needed[^}]*\\}\\}', flags=re.DOTALL)\ncn_pattern = re.compile(r'\\{\\{cn\\}\\}', flags=re.DOTALL)\ncurly_braces_pattern = re.compile(r'\\{\\{[^}]+\\}\\}', flags=re.DOTALL)\nwhitespace_pattern = re.compile(r'\\s+')","metadata":{"execution":{"iopub.status.busy":"2024-02-19T21:32:50.623707Z","iopub.execute_input":"2024-02-19T21:32:50.624206Z","iopub.status.idle":"2024-02-19T21:32:50.632713Z","shell.execute_reply.started":"2024-02-19T21:32:50.624176Z","shell.execute_reply":"2024-02-19T21:32:50.631817Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"The `all-MiniLM-L6-v2` model can handle a maximum sequence size of 256 words. As Wikipedia articles are often longer, I use the `text_to_sentences` function from`blingfire` to preprocess the data. This function takes a string (representing a Wikipedia article) and breaks up the article by sentences. The sentences are then saved into the `wikipedia_processed_*.txt` files locally line-by-line. After, I can simply utilize the Huggingface datasets library. ","metadata":{}},{"cell_type":"code","source":"def preprocess_article(text: str) -> str:\n    # Remove infoboxes and sidebars\n    text = infobox_pattern.sub('', text)\n    text = sidebar_pattern.sub('', text)\n    \n    # Simplify links - keep the text of the link only\n    text = link_pattern.sub(r'\\2', text)\n    \n    # Remove sections that start with == References ==, == External links ==, etc.\n    text = references_pattern.sub('', text)\n    \n    # Optional: Remove citation needed and similar templates\n    text = citation_needed_pattern.sub('', text)\n    text = cn_pattern.sub('', text)  # Short form of citation needed\n    \n    # Remove any remaining curly braces content (catch-all for other templates)\n    text = curly_braces_pattern.sub('', text)\n    \n    # Normalize whitespace to a single space\n    text = whitespace_pattern.sub(' ', text).strip()\n    \n    return text\n\ndef process_article(article_text: str, min_len: int, max_len: int) -> str:\n    # Preprocess the article text with regex\n    article_text = preprocess_article(article_text)\n    \n    # Skip the processing if there is no relavant sentences\n    if not article_text:\n        return \"\"\n    \n    # Initialize an empty list to hold sentences that meet the length criteria.\n    proper_sentences = []\n    \n    # Segment the preprocessed article text into sentences and obtain their offsets (start and end positions).\n    _, offsets = bf.text_to_sentences_and_offsets(article_text)\n    \n    for o in offsets:\n        # Check if the length of the current sentence (calculated as end position - start position)\n        # falls within the specified minimum and maximum length bounds.\n        if not min_len <= o[1] - o[0] <= max_len:\n            # If the sentence does not meet the length criteria, skip to the next iteration.\n            continue\n        \n        # Extract the sentence from the article text using the start and end positions from 'o'.\n        sentence = article_text[o[0]:o[1]]\n        \n        # Add the sentence that meets the length criteria to the list of proper sentences.\n        proper_sentences.append(sentence)\n    \n    # Join the proper sentences into a single string, separated by newline characters,\n    # and return this string. This results in a string where each sentence is on a new line,\n    # assuming it met the length criteria.\n    return '\\n'.join(proper_sentences)\n    \ndef process_article_wrapper(args):\n    return process_article(*args)\n\ndef process_wikipedia_dataset(wiki_dataset, output_dir, articles_per_file=1_000_000, batch_size=100):\n    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n    file_count = 1\n    article_count = 0\n    out_f = gzip.open(f'{output_dir}/wikipedia_processed_{file_count}.txt.gz', 'wt', encoding='utf-8')\n\n    with ProcessPoolExecutor() as executor:\n        futures = {}\n        for article in tqdm(wiki_dataset, desc='Processing Articles'):\n            # Submit tasks as you iterate\n            future = executor.submit(process_article_wrapper, (article['text'], 32, 2048))\n            futures[future] = article['text']\n\n            # Process completed tasks in batches to save memory\n            if len(futures) >= batch_size:\n                for future in as_completed(futures):\n                    sentences = future.result()\n                    out_f.write(sentences + '\\n')\n                    article_count += 1\n\n                    if article_count >= articles_per_file:\n                        out_f.close()\n                        file_count += 1\n                        article_count = 0\n                        out_f = gzip.open(f'{output_dir}/wikipedia_processed_{file_count}.txt.gz', 'wt', encoding='utf-8')\n                    # Remove the future from the dictionary once processed\n                    del futures[future]\n                    break  # Break after processing one to check if more tasks should be added\n\n        # Process any remaining tasks\n        for future in as_completed(futures):\n            sentences = future.result()\n            out_f.write(sentences + '\\n')\n            article_count += 1\n\n            if article_count >= articles_per_file:\n                out_f.close()\n                file_count += 1\n                article_count = 0\n                out_f = gzip.open(f'{output_dir}/wikipedia_processed_{file_count}.txt.gz', 'wt', encoding='utf-8')\n\n    # Ensure the last file is closed\n    out_f.close()","metadata":{"execution":{"iopub.status.busy":"2024-02-19T21:32:50.634909Z","iopub.execute_input":"2024-02-19T21:32:50.63549Z","iopub.status.idle":"2024-02-19T21:32:50.654151Z","shell.execute_reply.started":"2024-02-19T21:32:50.635451Z","shell.execute_reply":"2024-02-19T21:32:50.653049Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Load the Wikipedia dataset\nwiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split='train')\nprint(f'Length of the Wikipedia dataset is {len(wiki_dataset):_} articles.')\n\n# Specify the working directory to save the processed files\noutput_dir = '/kaggle/working'\n# Process the Wikipedia dataset add them to .txt.gz files\nprocess_wikipedia_dataset(wiki_dataset, output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T21:32:50.655355Z","iopub.execute_input":"2024-02-19T21:32:50.655684Z","iopub.status.idle":"2024-02-20T00:42:47.784097Z","shell.execute_reply.started":"2024-02-19T21:32:50.655656Z","shell.execute_reply":"2024-02-20T00:42:47.782751Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/11.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"029da8b19bb042649cf099aeace31e99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/7.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4443e611c74348cb998bf9c671069df8"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset wikipedia/20220301.en (download: 19.18 GiB, generated: 18.88 GiB, post-processed: Unknown size, total: 38.07 GiB) to /root/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/15.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9106ce87d8f9497da81ac60ceb444a22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/20.3G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78ec4b98d4cc4db9b23874a160521640"}},"metadata":{}},{"name":"stdout","text":"Dataset wikipedia downloaded and prepared to /root/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559. Subsequent calls will reuse this data.\nLength of the Wikipedia dataset is 6_458_670 articles.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Articles:   0%|          | 0/6458670 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffa8c89f6ccb4f6c8df183d7e26bbee0"}},"metadata":{}}]},{"cell_type":"markdown","source":"### The resulting files can be saved in a Kaggle dataset.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}