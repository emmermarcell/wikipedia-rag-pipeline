{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/emmermarcell/create-a-wikipedia-corpus?scriptVersionId=160121848\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Create a Wikipedia corpus\n\nThe aim of this notebook is to preprocess an existing wikipedia dump and create a wiikpedia corpus can can be used for NLP tasks. Since the dataset is quite large I will make use of the memory mapping between the RAM and the filesystems storage done by the the [Hugging Face Datasets library][1]. Under the hood, it utilizes the Apache Arrow memory format and pyarrow library. Unfortunately the [`wikipedia`][2] dataset is not streamable so I stick to iterating through it.\n\nI used the following article as a starting point for implementing a RAG pipeline:\n\n* [Steven van de Graaf - Pre-processing a Wikipedia dump for NLP model training — a write-up][3]\n\n[1]: https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt\n[2]: https://huggingface.co/datasets/wikipedia\n[3]: https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67","metadata":{}},{"cell_type":"code","source":"!pip install blingfire","metadata":{"execution":{"iopub.status.busy":"2024-01-23T11:21:59.691348Z","iopub.execute_input":"2024-01-23T11:21:59.691769Z","iopub.status.idle":"2024-01-23T11:22:11.249164Z","shell.execute_reply.started":"2024-01-23T11:21:59.691745Z","shell.execute_reply":"2024-01-23T11:22:11.248229Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting blingfire\n  Downloading blingfire-0.1.8-py3-none-any.whl (42.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: blingfire\nSuccessfully installed blingfire-0.1.8\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport gzip\nimport gc    # Garbage collector\nfrom tqdm.auto import tqdm\n\nfrom blingfire import text_to_sentences\nimport numpy as np\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-23T11:22:11.251398Z","iopub.execute_input":"2024-01-23T11:22:11.251705Z","iopub.status.idle":"2024-01-23T11:22:12.438827Z","shell.execute_reply.started":"2024-01-23T11:22:11.251669Z","shell.execute_reply":"2024-01-23T11:22:12.438018Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"The `all-MiniLM-L6-v2` model can handle a maximum sequence size of 256 words. As Wikipedia articles are often longer, I use the `text_to_sentences` function from`blingfire` to preprocess the data. This function takes a string (representing a Wikipedia article) and breaks up the article by sentences. The sentences are then saved into the `wikipedia_processed_*.txt` files locally line-by-line. After, I can simply utilize the Huggingface datasets library. ","metadata":{}},{"cell_type":"code","source":"def process_wikipedia_dataset(wiki_dataset, output_dir, articles_per_file=1_000_000):\n    file_count = 1\n    article_count = 0\n\n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Open the first output file for writing in compressed format\n    out_f = gzip.open(f'{output_dir}/wikipedia_processed_{file_count}.txt.gz', 'wt', encoding='utf-8')\n\n    # Iterate through each article in the dataset\n    for article in tqdm(wiki_dataset, desc='Processing Articles'):\n        # Process the article into sentences and write them into file\n        sentences = text_to_sentences(article['text'])\n        out_f.write(sentences + '\\n')\n\n        article_count += 1\n        # Check if it's time to switch to a new file\n        if article_count >= articles_per_file:\n            out_f.close()\n            file_count += 1\n            article_count = 0\n            out_f = gzip.open(f'{output_dir}/wikipedia_processed_{file_count}.txt.gz', 'wt', encoding='utf-8')\n\n    # Close the last file\n    out_f.close()","metadata":{"execution":{"iopub.status.busy":"2024-01-23T11:22:12.439978Z","iopub.execute_input":"2024-01-23T11:22:12.44047Z","iopub.status.idle":"2024-01-23T11:22:12.446477Z","shell.execute_reply.started":"2024-01-23T11:22:12.440441Z","shell.execute_reply":"2024-01-23T11:22:12.445575Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Load the Wikipedia dataset\nwiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split='train')\nprint(f'Length of the Wikipedia dataset is {len(wiki_dataset)} articles.')\n\n# Specify the working directory to save the processed files\noutput_dir = '/kaggle/working'\n# Process the Wikipedia dataset and return the path to the processed files\nprocess_wikipedia_dataset(wiki_dataset, output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-01-23T11:22:12.448601Z","iopub.execute_input":"2024-01-23T11:22:12.448938Z","iopub.status.idle":"2024-01-23T12:28:28.52577Z","shell.execute_reply.started":"2024-01-23T11:22:12.448911Z","shell.execute_reply":"2024-01-23T12:28:28.524152Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/11.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ece629b31a4481d872d431ee94fb4ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/7.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62cf0bbd68d34d76950d049959bd2044"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset wikipedia/20220301.en (download: 19.18 GiB, generated: 18.88 GiB, post-processed: Unknown size, total: 38.07 GiB) to /root/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/15.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd59c74ec6a34dceb4ce428bb7d089d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/20.3G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dca69fd6ddd54fe5b07d6c7759f08515"}},"metadata":{}},{"name":"stdout","text":"Dataset wikipedia downloaded and prepared to /root/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559. Subsequent calls will reuse this data.\nLength of the Wikipedia dataset is 6458670 articles.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Articles:   0%|          | 0/6458670 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71d354c210db450db0cd93e0ef962c1b"}},"metadata":{}}]},{"cell_type":"markdown","source":"### The resulting files can be saved in a Kaggle dataset.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}