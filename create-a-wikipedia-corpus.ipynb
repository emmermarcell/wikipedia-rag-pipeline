{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/emmermarcell/create-a-wikipedia-corpus?scriptVersionId=160036335\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Create a Wikipedia corpus\n\nThe aim of this notebook is to preprocess an existing wikipedia dump and create a wiikpedia corpus can can be used for NLP tasks. Since the dataset is quite large I will make use of the memory mapping between the RAM and the filesystems storage done by the the [Hugging Face Datasets library][1]. Under the hood, it utilizes the Apache Arrow memory format and pyarrow library. Unfortunately the [`wikipedia`][2] dataset is not streamable so I stick to iterating through it.\n\nI used the following article as a starting point for implementing a RAG pipeline:\n\n* [Steven van de Graaf - Pre-processing a Wikipedia dump for NLP model training — a write-up][3]\n\n[1]: https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt\n[2]: https://huggingface.co/datasets/wikipedia\n[3]: https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67","metadata":{}},{"cell_type":"code","source":"!pip install blingfire","metadata":{"execution":{"iopub.status.busy":"2024-01-22T18:47:24.414551Z","iopub.execute_input":"2024-01-22T18:47:24.415529Z","iopub.status.idle":"2024-01-22T18:47:48.635572Z","shell.execute_reply.started":"2024-01-22T18:47:24.415494Z","shell.execute_reply":"2024-01-22T18:47:48.634103Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting blingfire\n  Downloading blingfire-0.1.8-py3-none-any.whl (42.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: blingfire\nSuccessfully installed blingfire-0.1.8\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport gzip\nimport gc    # Garbage collector\nfrom tqdm.auto import tqdm\n\nfrom blingfire import text_to_sentences\nimport numpy as np\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-22T18:47:48.639722Z","iopub.execute_input":"2024-01-22T18:47:48.640787Z","iopub.status.idle":"2024-01-22T18:47:50.543998Z","shell.execute_reply.started":"2024-01-22T18:47:48.640749Z","shell.execute_reply":"2024-01-22T18:47:50.542731Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"The `all-MiniLM-L6-v2` model can handle a maximum sequence size of 256 words. As Wikipedia articles are often longer, I use the `text_to_sentences` function from`blingfire` to preprocess the data. This function takes a string (representing a Wikipedia article) and breaks up the article by sentences. The sentences are then saved into the `wikipedia_processed_*.txt` files locally line-by-line. After, I can simply utilize the Huggingface datasets library. ","metadata":{}},{"cell_type":"code","source":"def process_wikipedia_dataset(wiki_dataset, output_dir, articles_per_file=1_000_000):\n    file_count = 1\n    article_count = 0\n\n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Open the first output file for writing in compressed format\n    out_f = gzip.open(f'{output_dir}/wikipedia_processed_{file_count}.txt.gz', 'wt', encoding='utf-8')\n\n    # Iterate through each article in the dataset\n    for article in tqdm(wiki_dataset, desc='Processing Articles'):\n        # Process the article into sentences and write them into file\n        sentences = text_to_sentences(article['text'])\n        out_f.write(sentences + '\\n')\n\n        article_count += 1\n        # Check if it's time to switch to a new file\n        if article_count >= articles_per_file:\n            out_f.close()\n            file_count += 1\n            article_count = 0\n            out_f = gzip.open(f'{output_dir}/wikipedia_processed_{file_count}.txt.gz', 'wt', encoding='utf-8')\n\n    # Close the last file\n    out_f.close()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T18:47:50.545314Z","iopub.execute_input":"2024-01-22T18:47:50.54585Z","iopub.status.idle":"2024-01-22T18:47:50.554757Z","shell.execute_reply.started":"2024-01-22T18:47:50.54582Z","shell.execute_reply":"2024-01-22T18:47:50.553316Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Load the Wikipedia dataset\nwiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split='train')\nprint(f'Length of the Wikipedia dataset is {len(wiki_dataset)} articles.')\n\n# Specify the working directory to save the processed files\noutput_dir = '/kaggle/working'\n# Process the Wikipedia dataset and return the path to the processed files\nprocess_wikipedia_dataset(wiki_dataset, output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T18:47:50.55645Z","iopub.execute_input":"2024-01-22T18:47:50.557095Z","iopub.status.idle":"2024-01-22T20:16:49.914059Z","shell.execute_reply.started":"2024-01-22T18:47:50.557063Z","shell.execute_reply":"2024-01-22T20:16:49.912596Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/11.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19470b55ba4e4ef7806b4848f85a865e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/7.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a1bc0269d0146fdaa9e3f349bd2de21"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset wikipedia/20220301.en (download: 19.18 GiB, generated: 18.88 GiB, post-processed: Unknown size, total: 38.07 GiB) to /root/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/15.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b775a20f4f574aa8a882682670d1d2b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/20.3G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e44ad98756240a1a9d9da4ad16d7528"}},"metadata":{}},{"name":"stdout","text":"Dataset wikipedia downloaded and prepared to /root/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559. Subsequent calls will reuse this data.\nLength of the Wikipedia dataset is 6458670 articles.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Articles:   0%|          | 0/6458670 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57348aa9f1a94311af5f5fa61ce17027"}},"metadata":{}}]},{"cell_type":"markdown","source":"### The resulting files can be saved in a Kaggle dataset.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nwiki_corpus_path = '/kaggle/working/wikipedia_processed_1.txt.gz'\n# Load in the processed articles into a new Hugging Face dataset\nprocessed_wiki_dataset = load_dataset(\"text\", data_files={\"train\": wiki_corpus_path}, split='train[:10%]')\n\nprocessed_wiki_dataset['text'][:10]\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-22T20:16:49.917842Z","iopub.execute_input":"2024-01-22T20:16:49.918947Z","iopub.status.idle":"2024-01-22T20:16:49.929199Z","shell.execute_reply.started":"2024-01-22T20:16:49.918912Z","shell.execute_reply":"2024-01-22T20:16:49.927818Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'\\nwiki_corpus_path = \\'/kaggle/working/wikipedia_processed_1.txt.gz\\'\\n# Load in the processed articles into a new Hugging Face dataset\\nprocessed_wiki_dataset = load_dataset(\"text\", data_files={\"train\": wiki_corpus_path}, split=\\'train[:10%]\\')\\n\\nprocessed_wiki_dataset[\\'text\\'][:10]\\n'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}