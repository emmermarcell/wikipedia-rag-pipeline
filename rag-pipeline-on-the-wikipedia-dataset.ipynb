{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/emmermarcell/rag-pipeline-on-the-wikipedia-dataset?scriptVersionId=159966917\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Implementing a RAG pipeline on the Wikipedia dataset\n\n\n\nThe embedding of the Wikipedia articles are done with the [`all-MiniLM-L6-v2`][4] model from the [Sentence-Transformers][5] library.  The strings are embedded into a $384$ dimensional vector space where a similarity search is performed by the `faiss.IndexFlatL2` index based on their Euclidean (L2) distance.\n\nThe notebook runs on 2xT4 GPUs that Kaggle provides.\nA great resource for training faiss on multiple GPUs can be found on the [faiss github site][2]. Furthermore, for computing embeddings on multiple GPUs I reference the [Sentence-Transformers github site][3].\n\nAfter the embedding and ranking of athe article chunks, I employ the [`distilbert-base-cased-distilled-squad`][9] Q&A pipeline, a fine-tuned version of the [`DistilBERT-base-cased`][10] model using (a second step of) knowledge distillation on the [`SQuAD v1.1`][11] dataset.\n\nI used the following articles as a starting point for implementing a RAG pipeline:\n\n* [Akriti Upadhyay - Implementing RAG with Langchain and Hugging Face][6]\n\n* [Vladimir Blagojevic - Ask Wikipedia ELI5-like Questions Using Long-Form Question Answering on Haystack][7]\n\n* [Steven van de Graaf - Pre-processing a Wikipedia dump for NLP model training — a write-up][12]\n\n[1]: https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt\n[2]: https://github.com/facebookresearch/faiss/blob/main/tutorial/python/5-Multiple-GPUs.py\n[3]: https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/computing-embeddings/computing_embeddings_multi_gpu.py\n[4]: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n[5]: https://www.sbert.net/\n[6]: https://medium.com/international-school-of-ai-data-science/implementing-rag-with-langchain-and-hugging-face-28e3ea66c5f7q=implementing+rag+with+langchain+and+huggingface&oq=implementing+rag+with+langchain+and+huggingface&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIKCAEQABiABBiiBDIKCAIQABiABBiiBNIBCDc2MzFqMGo3qAIAsAIA&client=ubuntu-chr&sourceid=chrome&ie=UTF-8\n[7]: https://medium.com/international-school-of-ai-data-science/implementing-rag-with-langchain-and-hugging-face-28e3ea66c5f7\n[8]: https://huggingface.co/datasets/wikipedia\n[9]: https://huggingface.co/distilbert-base-cased-distilled-squad\n[10]: https://huggingface.co/distilbert-base-cased\n[11]: https://huggingface.co/datasets/squad\n[12]: https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67","metadata":{}},{"cell_type":"code","source":"!pip install sentence_transformers\n!pip install faiss-gpu","metadata":{"execution":{"iopub.status.busy":"2024-01-22T09:28:04.115087Z","iopub.execute_input":"2024-01-22T09:28:04.115352Z","iopub.status.idle":"2024-01-22T09:28:35.525601Z","shell.execute_reply.started":"2024-01-22T09:28:04.115328Z","shell.execute_reply":"2024-01-22T09:28:35.524195Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.36.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.15.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.24.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.11.4)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.20.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\nBuilding wheels for collected packages: sentence_transformers\n  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=a7bd3e86a2ca7cb8914099010657ed7ce2e9a45667efa7b18dc1785559870bfa\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built sentence_transformers\nInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-2.2.2\nCollecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import gc    # Garbage collector\nimport logging\nfrom tqdm.auto import tqdm\n\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import pipeline\nfrom sentence_transformers import SentenceTransformer, LoggingHandler\nimport faiss\n\n\nlogging.basicConfig(\n    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, handlers=[LoggingHandler()]\n)\n\n# Ensure you have a GPU available\nngpus = faiss.get_num_gpus()\nprint(\"number of GPUs:\", ngpus)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T09:28:35.528249Z","iopub.execute_input":"2024-01-22T09:28:35.529045Z","iopub.status.idle":"2024-01-22T09:28:54.166279Z","shell.execute_reply.started":"2024-01-22T09:28:35.529Z","shell.execute_reply":"2024-01-22T09:28:54.165291Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"number of GPUs: 2\n","output_type":"stream"}]},{"cell_type":"code","source":"# Important, you need to shield your code with if __name__. Otherwise, CUDA runs into issues when spawning new processes.\nif __name__ == \"__main__\":\n    wiki_corpus_path = '/kaggle/input/wikipedia-sentences/wikisent2.txt'\n    # Load in the processed articles into a new Hugging Face dataset\n    processed_wiki_dataset = load_dataset(\"text\", data_files={\"train\": wiki_corpus_path}, split='train[:10%]')\n    print(f'Length of the Wikipedia dataset is {len(processed_wiki_dataset)} articles.')\n    \n    # Define the sentence transformer model\n    model_name = \"all-MiniLM-L6-v2\"\n    model = SentenceTransformer(model_name)\n    embedding_dim = model.get_sentence_embedding_dimension()    # Get the embedding dimension\n    max_seq_len = model.max_seq_length    # Maximum sequence length in words\n    print(f'The embedding dimension of the all-MiniLM-L6-v2 model is {embedding_dim}.')\n    print(f\"Max sequence lenght of the {model_name} model is {max_seq_len}.\")\n    \n    # Initialize a FAISS index (for CPU)\n    cpu_index = faiss.IndexFlatL2(embedding_dim)\n\n    # Initialize GPU resources for FAISS\n    gpu_index = faiss.index_cpu_to_all_gpus(  # build the index\n        cpu_index\n    )        \n        \n    # Start the multi-process pool on all available CUDA devices\n    pool = model.start_multi_process_pool()\n    \n    # Batch processing with tqdm progress bar\n    # Define batch size based on the system's memory capacity\n    batch_size = 2**10\n    total_batches = len(processed_wiki_dataset['text']) // batch_size + (0 if len(processed_wiki_dataset['text']) % batch_size == 0 else 1)\n    \n    for i in tqdm(range(0, len(processed_wiki_dataset['text']), batch_size), total=total_batches, desc=\"Processing Batches\"):\n        # Take the next batch of articles\n        batch_texts = processed_wiki_dataset['text'][i:i + batch_size]\n        # Compute the embeddings using the multi-process pool\n        batch_embeddings = model.encode_multi_process(batch_texts, pool)\n        # Add embeddings to the GPU index\n        gpu_index.add(batch_embeddings)\n        \n        # Memory management\n        del batch_embeddings\n        gc.collect()\n        \n    # Function to search for relevant articles using GPU\n    def search_wiki_articles(question):\n        question_embedding = model.encode_multi_process(question, pool)\n        distances, indices = gpu_index.search(question_embedding, k=5)\n        return [processed_wiki_dataset['text'][i] for i in indices[0]]\n\n    # State business questions\n    questions = [\n        'What services does KPMG offer to its clients?',\n        'What are the key considerations when assessing internal controls during an audit?',\n        'How do you stay updated on changes in tax laws and regulations affecting clients?',\n        \"What steps do you take to understand a client's business before initiating a consulting project?\",\n        'What due diligence processes are crucial for evaluating the financial health of a potential acquisition?',\n    ]\n    \n    relevant_article_chunks = [search_wiki_articles(question) for question in questions]\n    \n    # Example\n    print(f'Question:\\n{questions[0]}')\n    print(f'Relevant Wikipedia article chunks:\\n{relevant_article_chunks[0]}')\n    \n    # Optional: Stop the processes in the pool\n    model.stop_multi_process_pool(pool)\n    \n    # (Optional) Save the faiss index\n    # faiss.write_index(gpu_index, 'Wikipedia_FlatL2.index')","metadata":{"execution":{"iopub.status.busy":"2024-01-22T09:28:54.167637Z","iopub.execute_input":"2024-01-22T09:28:54.168392Z","iopub.status.idle":"2024-01-22T09:54:24.874311Z","shell.execute_reply.started":"2024-01-22T09:28:54.168358Z","shell.execute_reply":"2024-01-22T09:54:24.873326Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-7a1456ff65230512/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25bc74a7d75a4778b0a7353124df894a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8eb6c8b0a184353be2a5239eeadcd80"}},"metadata":{}},{"name":"stdout","text":"Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-7a1456ff65230512/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8. Subsequent calls will reuse this data.\nLength of the Wikipedia dataset is 787182 articles.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34d1980c9fab481eba60184ddeede769"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccd195c5ed324bfab185a8ee2659e090"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca5729ce45eb4abab6f9d5851426f92c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ba110fb8c364e0fb2e3d6fb8405e089"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ce917e4a1484796af1a56f23516f477"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1ed6f3039bc40208a3f6524b5954d4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eba2af16d0a43afb267631aadfb1708"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6c5262a3442445aa0f2e1a305f641eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f51301079c8c4e6ca6ba4fbaf6537b2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a317613bb9e44612a55410fa1601ed1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f94cb9b5de2479e9a430d68a85c99e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f25a77420974446ac1a8772f958e322"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3dff878575d4280b5e2eab5b1134f5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a12528c508747e79382ed4cf58afd99"}},"metadata":{}},{"name":"stdout","text":"The embedding dimension of the all-MiniLM-L6-v2 model is 384.\nMax sequence lenght of the all-MiniLM-L6-v2 model is 256.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Batches:   0%|          | 0/769 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4898e1a6c7c6450e91d260502a3464cb"}},"metadata":{}},{"name":"stdout","text":"Question:\nWhat services does KPMG offer to its clients?\nRelevant Wikipedia article chunks:\n['A misprint caused the U to change to a W and the name stuck.', \"According to this principle - simplifying - a word W precedes a word W' if and only if W is contained in a node Q that asymmetrically c-commands a node R containing W'.\", 'ALGOL W is a programming language.', 'Black and white, often abbreviated B/W or B&W, and hyphenated black-and-white when used as an adjective, is any of several monochrome forms in visual arts.', \"A year later, following the United States' entry into World War I, the WLI served in the United States Army overseas in the 105th Ammunition Train, 55th Field Artillery Brigade, 30th Division.\"]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The maximum length of tokens the we can feed to the `distilbert-base-cased-distilled-squad` tokenizer before truncation is 512. Therefore it is more than enough to search for the k=5 kNN of article chunks for a given question using the faiss index.","metadata":{}},{"cell_type":"code","source":"# Load the Q&A pipeline\nqa_model_name = 'distilbert-base-cased-distilled-squad'\nquestion_answerer = pipeline('question-answering', model=qa_model_name, tokenizer=qa_model_name)\nprint(f'The maximum length of tokens the we can feed to the tokenizer before truncation is {question_answerer.tokenizer.model_max_length}.')\n\n# Function to perform question-answering given a question and a list of documents\ndef answer_question(question, article_chunks):\n    # Combine the article chunks into a single string\n    context = ' '.join(article_chunks)\n\n    # Perform question-answering\n    result = question_answerer(question=question, context=context)\n\n    return result\n\n# Answer the questions\nresults = [answer_question(questions[idx], relevant_article_chunks[idx]) for idx in range(len(questions))]\n\n#for result in results:\n#    print(f\"Answer: '{results[idx]['answer']}', score: {round(results[idx]['score'], 4)}, start: {results[idx]['start']}, end: {results[idx]['end']}\")\n    \nfor idx in range(len(questions)):\n    print(f'Question:\\n{questions[idx]}')\n    print(f\"Answer:\\n'{results[idx]['answer']}',\\nscore:{round(results[idx]['score'], 4)}, start:{results[idx]['start']}, end:{results[idx]['end']}\")\n    print('='*30)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T09:54:24.876733Z","iopub.execute_input":"2024-01-22T09:54:24.877049Z","iopub.status.idle":"2024-01-22T09:54:33.521265Z","shell.execute_reply.started":"2024-01-22T09:54:24.877022Z","shell.execute_reply":"2024-01-22T09:54:33.520204Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"def576e65cac4fbaa7e25bfcb9920a02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"772b6b4ce34e474f8488f4d42266f355"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18e092aa12db4230a09e120fbca513a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffd69661126c4c738fac2ed2ae22c09d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"068f2fb1d80d46abb6c21bd73f76c119"}},"metadata":{}},{"name":"stdout","text":"The maximum length of tokens the we can feed to the tokenizer before truncation is 512.\nQuestion:\nWhat services does KPMG offer to its clients?\nAnswer:\n'visual arts',\nscore:0.0185, start:408, end:419\n==============================\nQuestion:\nWhat are the key considerations when assessing internal controls during an audit?\nAnswer:\n'if W is contained in a node Q',\nscore:0.0051, start:145, end:174\n==============================\nQuestion:\nHow do you stay updated on changes in tax laws and regulations affecting clients?\nAnswer:\n'they appear to be in a vertical or horizontal letter H.',\nscore:0.0078, start:121, end:176\n==============================\nQuestion:\nWhat steps do you take to understand a client's business before initiating a consulting project?\nAnswer:\n'if and only if W is contained in a node Q',\nscore:0.0702, start:133, end:174\n==============================\nQuestion:\nWhat due diligence processes are crucial for evaluating the financial health of a potential acquisition?\nAnswer:\n'A misprint',\nscore:0.0475, start:0, end:10\n==============================\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluation of the pipeline\n\nFor evaluation a Q&A pipeline, one an use the [Official Evaluation Script][1] of the [SQuAD v2.0][2] dataset. I inclued a part of the script that I can evaluate the Exact and the F1 score of a pipeline on this dataset.\n\n[1]: https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n[2]: https://rajpurkar.github.io/SQuAD-explorer/","metadata":{}},{"cell_type":"code","source":"\"\"\"Official evaluation script for SQuAD version 2.0.\n\nIn addition to basic functionality, we also compute additional statistics and\nplot precision-recall curves if an additional na_prob.json file is provided.\nThis file is expected to map question ID's to the model's predicted probability\nthat a question is unanswerable.\n\"\"\"\nimport argparse\nimport collections\nimport json\nimport numpy as np\nimport os\nimport re\nimport string\nimport sys\n\nOPTS = None\n\ndef parse_args():\n    parser = argparse.ArgumentParser('Official evaluation script for SQuAD version 2.0.')\n    parser.add_argument('data_file', metavar='data.json', help='Input data JSON file.')\n    parser.add_argument('pred_file', metavar='pred.json', help='Model predictions.')\n    parser.add_argument('--out-file', '-o', metavar='eval.json',\n                        help='Write accuracy metrics to file (default is stdout).')\n    parser.add_argument('--na-prob-file', '-n', metavar='na_prob.json',\n                        help='Model estimates of probability of no answer.')\n    parser.add_argument('--na-prob-thresh', '-t', type=float, default=1.0,\n                        help='Predict \"\" if no-answer probability exceeds this (default = 1.0).')\n    parser.add_argument('--out-image-dir', '-p', metavar='out_images', default=None,\n                        help='Save precision-recall curves to directory.')\n    parser.add_argument('--verbose', '-v', action='store_true')\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n    return parser.parse_args()\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    def remove_articles(text):\n        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n        return re.sub(regex, ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1","metadata":{"execution":{"iopub.status.busy":"2024-01-22T09:54:33.522777Z","iopub.execute_input":"2024-01-22T09:54:33.523461Z","iopub.status.idle":"2024-01-22T09:54:33.539652Z","shell.execute_reply.started":"2024-01-22T09:54:33.523422Z","shell.execute_reply":"2024-01-22T09:54:33.538684Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Load in the squad dataset\nsquad_dataset = dataset = load_dataset('squad', split='train')\n\n# Example use\nquestion = squad_dataset['question'][0]\ncontext = squad_dataset['context'][0]\nanswer = squad_dataset['answers'][0]\n\n# Pedict the answer using the distilbert Q&A pipeline\nresult = question_answerer(question=question, context=context)\n\n# Compute the evluation scores\nexact_score = compute_exact(a_gold=answer['text'][0], a_pred=result['answer'])\nf1_score = compute_f1(a_gold=answer['text'][0], a_pred=result['answer'])\n\n\nprint(f'Question:\\n{question}')\nprint(f\"Predited Answer:\\n{result['answer']}\")\nprint(f\"Proper answer accoding to SQuAD:\\n{answer['text'][0]}\")\nprint(f'F1: {f1_score},\\tE: {exact_score}')","metadata":{"execution":{"iopub.status.busy":"2024-01-22T09:54:33.540815Z","iopub.execute_input":"2024-01-22T09:54:33.541132Z","iopub.status.idle":"2024-01-22T09:54:49.759601Z","shell.execute_reply.started":"2024-01-22T09:54:33.541109Z","shell.execute_reply":"2024-01-22T09:54:49.758568Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22d5620e3c89409e9aa3003a40bd8e71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb2dc77b71134b4c851b985e83db206c"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49c294c89ca24258b3a740236667bf80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/8.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8e524a2f3534d77961cd3ef0eb6f1e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cec2bc4bcd8d4e4d9af2ef15efdf29fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84a5cdedb7dc4490bcbee10a076049b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\nQuestion:\nTo whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\nPredited Answer:\nSaint Bernadette Soubirous\nProper answer accoding to SQuAD:\nSaint Bernadette Soubirous\nF1: 1.0,\tE: 1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Making the code accessible for end-users\n\nCan be done e.g. with a simple GUI. Some references for how it can be done:\n\n* [Kamalraj M M - Step By Step Guide to Integrate LLM with GUI: Improving Performance Of GUI with LLM][1]\n* [PySimpleGUI][2]\n* Making a simple website from scratch using Flask or Django\n\n[1]: https://www.youtube.com/watch?v=nWi8yM4bCmM\n[2]: https://www.pysimplegui.org/en/latest/#jump-start","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}