{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/emmermarcell/rag-pipeline-on-the-wikipedia-dataset?scriptVersionId=159940868\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Implementing a RAG pipeline on the Wikipedia dataset\n\n\n\nThe embedding of the Wikipedia articles are done with the [`all-MiniLM-L6-v2`][4] model from the [Sentence-Transformers][5] library.  The strings are embedded into a $384$ dimensional vector space where a similarity search is performed by the `faiss.IndexFlatL2` index based on their Euclidean (L2) distance.\n\nThe notebook runs on 2xT4 GPUs that Kaggle provides.\nA great resource for training faiss on multiple GPUs can be found on the [faiss github site][2]. Furthermore, for computing embeddings on multiple GPUs I reference the [Sentence-Transformers github site][3].\n\nAfter the embedding and ranking of athe article chunks, I employ the [`distilbert-base-cased-distilled-squad`][9] Q&A pipeline, a fine-tuned version of the [`DistilBERT-base-cased`][10] model using (a second step of) knowledge distillation on the [`SQuAD v1.1`][11] dataset.\n\nI used the following articles as a starting point for implementing a RAG pipeline:\n\n* [Akriti Upadhyay - Implementing RAG with Langchain and Hugging Face][6]\n\n* [Vladimir Blagojevic - Ask Wikipedia ELI5-like Questions Using Long-Form Question Answering on Haystack][7]\n\n* [Steven van de Graaf - Pre-processing a Wikipedia dump for NLP model training â€” a write-up][12]\n\n[1]: https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt\n[2]: https://github.com/facebookresearch/faiss/blob/main/tutorial/python/5-Multiple-GPUs.py\n[3]: https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/computing-embeddings/computing_embeddings_multi_gpu.py\n[4]: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n[5]: https://www.sbert.net/\n[6]: https://medium.com/international-school-of-ai-data-science/implementing-rag-with-langchain-and-hugging-face-28e3ea66c5f7q=implementing+rag+with+langchain+and+huggingface&oq=implementing+rag+with+langchain+and+huggingface&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIKCAEQABiABBiiBDIKCAIQABiABBiiBNIBCDc2MzFqMGo3qAIAsAIA&client=ubuntu-chr&sourceid=chrome&ie=UTF-8\n[7]: https://medium.com/international-school-of-ai-data-science/implementing-rag-with-langchain-and-hugging-face-28e3ea66c5f7\n[8]: https://huggingface.co/datasets/wikipedia\n[9]: https://huggingface.co/distilbert-base-cased-distilled-squad\n[10]: https://huggingface.co/distilbert-base-cased\n[11]: https://huggingface.co/datasets/squad\n[12]: https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67","metadata":{}},{"cell_type":"code","source":"!pip install sentence_transformers\n!pip install faiss-gpu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc    # Garbage collector\nimport logging\nfrom tqdm.auto import tqdm\n\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import pipeline\nfrom sentence_transformers import SentenceTransformer, LoggingHandler\nimport faiss\n\n\nlogging.basicConfig(\n    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, handlers=[LoggingHandler()]\n)\n\n# Ensure you have a GPU available\nngpus = faiss.get_num_gpus()\nprint(\"number of GPUs:\", ngpus)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Important, you need to shield your code with if __name__. Otherwise, CUDA runs into issues when spawning new processes.\nif __name__ == \"__main__\":\n    wiki_corpus_path = '/kaggle/input/wikipedia-sentences/wikisent2.txt'\n    # Load in the processed articles into a new Hugging Face dataset\n    processed_wiki_dataset = load_dataset(\"text\", data_files={\"train\": wiki_corpus_path})\n    print(f'Length of the Wikipedia dataset is {len(processed_wiki_dataset)} articles.')\n    \n    # Define the sentence transformer model\n    model_name = \"all-MiniLM-L6-v2\"\n    model = SentenceTransformer(model_name)\n    embedding_dim = model.get_sentence_embedding_dimension()    # Get the embedding dimension\n    max_seq_len = model.max_seq_length    # Maximum sequence length in words\n    print(f'The embedding dimension of the all-MiniLM-L6-v2 model is {embedding_dim}.')\n    print(f\"Max sequence lenght of the {model_name} model is {max_seq_len}.\")\n    \n    # Initialize a FAISS index (for CPU)\n    cpu_index = faiss.IndexFlatL2(embedding_dim)\n\n    # Initialize GPU resources for FAISS\n    gpu_index = faiss.index_cpu_to_all_gpus(  # build the index\n        cpu_index\n    )        \n        \n    # Start the multi-process pool on all available CUDA devices\n    pool = model.start_multi_process_pool()\n    \n    # Batch processing with tqdm progress bar\n    # Define batch size based on the system's memory capacity\n    batch_size = 2**13\n    total_batches = len(processed_wiki_dataset['train']['text']) // batch_size + (0 if len(processed_wiki_dataset['train']['text']) % batch_size == 0 else 1)\n    \n    for i in tqdm(range(0, len(processed_wiki_dataset['train']['text']), batch_size), total=total_batches, desc=\"Processing Batches\"):\n        # Take the next batch of articles\n        batch_texts = processed_wiki_dataset['train']['text'][i:i + batch_size]\n        # Compute the embeddings using the multi-process pool\n        batch_embeddings = model.encode_multi_process(batch_texts, pool)\n        # Add embeddings to the GPU index\n        gpu_index.add(batch_embeddings)\n        \n        # Memory management\n        del batch_embeddings\n        gc.collect()\n        \n    # Function to search for relevant articles using GPU\n    def search_wiki_articles(question):\n        question_embedding = model.encode_multi_process(question, pool)\n        distances, indices = gpu_index.search(question_embedding, k=5)\n        return [processed_wiki_dataset['train']['text'][i] for i in indices[0]]\n\n    # State business questions\n    questions = [\n        'What services does KPMG offer to its clients?',\n        'What are the key considerations when assessing internal controls during an audit?',\n        'How do you stay updated on changes in tax laws and regulations affecting clients?',\n        \"What steps do you take to understand a client's business before initiating a consulting project?\",\n        'What due diligence processes are crucial for evaluating the financial health of a potential acquisition?',\n    ]\n    \n    relevant_article_chunks = [search_wiki_articles(question) for question in questions]\n    \n    # Example\n    print(f'Question:\\n{questions[0]}')\n    print(f'Relevant Wikipedia article chunks:\\n{relevant_article_chunks[0]}')\n    \n    # Optional: Stop the processes in the pool\n    model.stop_multi_process_pool(pool)\n    \n    # (Optional) Save the faiss index\n    # faiss.write_index(gpu_index, 'Wikipedia_FlatL2.index')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The maximum length of tokens the we can feed to the `distilbert-base-cased-distilled-squad` tokenizer before truncation is 512. Therefore it is more than enough to search for the k=5 kNN of article chunks for a given question using the faiss index.","metadata":{}},{"cell_type":"code","source":"# Load the Q&A pipeline\nqa_model_name = 'distilbert-base-cased-distilled-squad'\nquestion_answerer = pipeline('question-answering', model=qa_model_name, tokenizer=qa_model_name)\nprint(f'The maximum length of tokens the we can feed to the tokenizer before truncation is {question_answerer.tokenizer.model_max_length}.')\n\n# Function to perform question-answering given a question and a list of documents\ndef answer_question(question, article_chunks):\n    # Combine the article chunks into a single string\n    context = ' '.join(article_chunks)\n\n    # Perform question-answering\n    result = question_answerer(question=question, context=context)\n\n    return result\n\n# Answer the questions\nresults = [answer_question(questions[idx], relevant_article_chunks[idx]) for idx in range(len(questions))]\n\n#for result in results:\n#    print(f\"Answer: '{results[idx]['answer']}', score: {round(results[idx]['score'], 4)}, start: {results[idx]['start']}, end: {results[idx]['end']}\")\n    \nfor idx in range(len(questions)):\n    print(f'Question:\\n{questions[idx]}')\n    print(f\"Answer:\\n'{results[idx]['answer']}',\\nscore:{round(results[idx]['score'], 4)}, start:{results[idx]['start']}, end:{results[idx]['end']}\")\n    print('='*30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation of the pipeline\n\nFor evaluation a Q&A pipeline, one an use the [Official Evaluation Script][1] of the [SQuAD v2.0][2] dataset. I inclued a part of the script that I can evaluate the Exact and the F1 score of a pipeline on this dataset.\n\n[1]: https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n[2]: https://rajpurkar.github.io/SQuAD-explorer/","metadata":{}},{"cell_type":"code","source":"\"\"\"Official evaluation script for SQuAD version 2.0.\n\nIn addition to basic functionality, we also compute additional statistics and\nplot precision-recall curves if an additional na_prob.json file is provided.\nThis file is expected to map question ID's to the model's predicted probability\nthat a question is unanswerable.\n\"\"\"\nimport argparse\nimport collections\nimport json\nimport numpy as np\nimport os\nimport re\nimport string\nimport sys\n\nOPTS = None\n\ndef parse_args():\n    parser = argparse.ArgumentParser('Official evaluation script for SQuAD version 2.0.')\n    parser.add_argument('data_file', metavar='data.json', help='Input data JSON file.')\n    parser.add_argument('pred_file', metavar='pred.json', help='Model predictions.')\n    parser.add_argument('--out-file', '-o', metavar='eval.json',\n                        help='Write accuracy metrics to file (default is stdout).')\n    parser.add_argument('--na-prob-file', '-n', metavar='na_prob.json',\n                        help='Model estimates of probability of no answer.')\n    parser.add_argument('--na-prob-thresh', '-t', type=float, default=1.0,\n                        help='Predict \"\" if no-answer probability exceeds this (default = 1.0).')\n    parser.add_argument('--out-image-dir', '-p', metavar='out_images', default=None,\n                        help='Save precision-recall curves to directory.')\n    parser.add_argument('--verbose', '-v', action='store_true')\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n    return parser.parse_args()\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    def remove_articles(text):\n        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n        return re.sub(regex, ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load in the squad dataset\nsquad_dataset = dataset = load_dataset('squad', split='train')\n\n# Example use\nquestion = squad_dataset['question'][0]\ncontext = squad_dataset['context'][0]\nanswer = squad_dataset['answers'][0]\n\n# Pedict the answer using the distilbert Q&A pipeline\nresult = question_answerer(question=question, context=context)\n\n# Compute the evluation scores\nexact_score = compute_exact(a_gold=answer['text'][0], a_pred=result['answer'])\nf1_score = compute_f1(a_gold=answer['text'][0], a_pred=result['answer'])\n\n\nprint(f'Question:\\n{question}')\nprint(f\"Predited Answer:\\n{result['answer']}\")\nprint(f\"Proper answer accoding to SQuAD:\\n{answer['text'][0]}\")\nprint(f'F1: {f1_score},\\tE: {exact_score}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Making the code accessible for end-users\n\nCan be done e.g. with a simple GUI. Some references for how it can be done:\n\n* [Kamalraj M M - Step By Step Guide to Integrate LLM with GUI: Improving Performance Of GUI with LLM][1]\n* [PySimpleGUI][2]\n* Making a simple website from scratch using Flask or Django\n\n[1]: https://www.youtube.com/watch?v=nWi8yM4bCmM\n[2]: https://www.pysimplegui.org/en/latest/#jump-start","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}