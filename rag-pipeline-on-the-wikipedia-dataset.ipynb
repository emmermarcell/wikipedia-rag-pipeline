{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/emmermarcell/rag-pipeline-on-the-wikipedia-dataset?scriptVersionId=159749080\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Implementing a RAG pipeline on the Wikipedia dataset\n\nSince the dataset is quite large I will make use of the memory mapping between the RAM and the filesystems storage done by the the [Hugging Face Datasets library][1]. Under the hood, it utilizes the Apache Arrow memory format and pyarrow library. Unfortunately the [`wikipedia`][8] dataset is not streamable so I stick to iterating through it.\n\nThe embedding of the Wikipedia articles are done with the [`all-MiniLM-L6-v2`][4] model from the [Sentence-Transformers][5] library.  The strings are embedded into a $384$ dimensional vector space where a similarity search is performed by the `faiss.IndexFlatL2` index based on their Euclidean (L2) distance.\n\nThe notebook runs on 2xT4 GPUs that Kaggle provides.\nA great resource for training faiss on multiple GPUs can be found on the [faiss github site][2]. Furthermore, for computing embeddings on multiple GPUs I reference the [Sentence-Transformers github site][3].\n\nAfter the embedding and ranking of athe article chunks, I employ the [`distilbert-base-cased-distilled-squad`][9] Q&A pipeline, a fine-tuned version of the [`DistilBERT-base-cased`][10] model using (a second step of) knowledge distillation on the [`SQuAD v1.1`][11] dataset.\n\nI used the following articles as a starting point for implementing a RAG pipeline:\n\n* [Akriti Upadhyay - Implementing RAG with Langchain and Hugging Face][6]\n\n* [Vladimir Blagojevic - Ask Wikipedia ELI5-like Questions Using Long-Form Question Answering on Haystack][7]\n\n[1]: https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt\n[2]: https://github.com/facebookresearch/faiss/blob/main/tutorial/python/5-Multiple-GPUs.py\n[3]: https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/computing-embeddings/computing_embeddings_multi_gpu.py\n[4]: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n[5]: https://www.sbert.net/\n[6]: https://medium.com/international-school-of-ai-data-science/implementing-rag-with-langchain-and-hugging-face-28e3ea66c5f7q=implementing+rag+with+langchain+and+huggingface&oq=implementing+rag+with+langchain+and+huggingface&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIKCAEQABiABBiiBDIKCAIQABiABBiiBNIBCDc2MzFqMGo3qAIAsAIA&client=ubuntu-chr&sourceid=chrome&ie=UTF-8\n[7]: https://towardsdatascience.com/ask-wikipedia-eli5-like-questions-using-long-form-question-answering-on-haystack-32cf1ca6c00e\n[8]: https://huggingface.co/datasets/wikipedia\n[9]: https://huggingface.co/distilbert-base-cased-distilled-squad\n[10]: https://huggingface.co/distilbert-base-cased\n[11]: https://huggingface.co/datasets/squad","metadata":{}},{"cell_type":"code","source":"!pip install sentence_transformers\n!pip install faiss-gpu","metadata":{"execution":{"iopub.status.busy":"2024-01-20T14:47:56.794451Z","iopub.execute_input":"2024-01-20T14:47:56.79474Z","iopub.status.idle":"2024-01-20T14:48:29.970796Z","shell.execute_reply.started":"2024-01-20T14:47:56.794712Z","shell.execute_reply":"2024-01-20T14:48:29.969593Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m963.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.36.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.15.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.24.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.11.4)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.20.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\nBuilding wheels for collected packages: sentence_transformers\n  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=e1f5e4eba8284c5d3de8bafdfec9741c1ef171681c5ca89f5a0f245456cf7b1b\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built sentence_transformers\nInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-2.2.2\nCollecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import gc    # Garbage collector\nimport logging\nimport time\nimport re\nfrom tqdm.auto import tqdm\nfrom nltk.tokenize import word_tokenize\nfrom concurrent.futures import ProcessPoolExecutor\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import pipeline\nfrom sentence_transformers import SentenceTransformer, LoggingHandler\nimport faiss\n\n\nlogging.basicConfig(\n    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, handlers=[LoggingHandler()]\n)\n\n\n# Ensure you have a GPU available\nngpus = faiss.get_num_gpus()\nprint(\"number of GPUs:\", ngpus)","metadata":{"execution":{"iopub.status.busy":"2024-01-20T14:48:29.973165Z","iopub.execute_input":"2024-01-20T14:48:29.973962Z","iopub.status.idle":"2024-01-20T14:48:58.971608Z","shell.execute_reply.started":"2024-01-20T14:48:29.97392Z","shell.execute_reply":"2024-01-20T14:48:58.970721Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"number of GPUs: 2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The `all-MiniLM-L6-v2` model can handle a maximum sequence size of 256 words. As Wikipedia articles are often longer, we use the `process_strings_and_combine_parallel` function to preprocess the data. This function takes a list of strings (representing Wikipedia articles) and returns another list of strings. It breaks up each string into chunks of words, and the size of each chunk is determined by the `chunk_size` parameter. The text processing is done with the `ProcessPoolExecutor`method from `concurrent.futures` that uses a pool of processes to execute calls asynchronously.","metadata":{}},{"cell_type":"code","source":"def combine_into_chunks(words_list, chunk_size):\n    # Generate chunks of the specified size from the words list\n    chunks = [words_list[i:i + chunk_size] for i in range(0, len(words_list), chunk_size)]\n    # Join each chunk into a single string and return the list of chunks\n    return [' '.join(chunk) for chunk in chunks]\n\ndef process_string(text, chunk_size):\n    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n    words_list = word_tokenize(cleaned_text)\n    return combine_into_chunks(words_list, chunk_size)\n\ndef process_strings_and_combine_parallel(string_list, chunk_size):\n    result_chunks = []\n\n    with ProcessPoolExecutor() as executor:\n        # Pass chunk_size as an argument to the process_string function\n        chunks_list = list(executor.map(process_string, string_list, [chunk_size] * len(string_list)))\n\n    for chunks in chunks_list:\n        result_chunks.extend(chunks)\n\n    return result_chunks","metadata":{"execution":{"iopub.status.busy":"2024-01-20T14:48:58.972601Z","iopub.execute_input":"2024-01-20T14:48:58.973146Z","iopub.status.idle":"2024-01-20T14:48:58.982184Z","shell.execute_reply.started":"2024-01-20T14:48:58.97312Z","shell.execute_reply":"2024-01-20T14:48:58.979671Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Important, you need to shield your code with if __name__. Otherwise, CUDA runs into issues when spawning new processes.\nif __name__ == \"__main__\":\n    # Define the sentence transformer model\n    model_name = \"all-MiniLM-L6-v2\"\n    model = SentenceTransformer(model_name)\n    embedding_dim = model.get_sentence_embedding_dimension()    # Get the embedding dimension\n    max_seq_len = model.max_seq_length    # Maximum sequence length in words\n    print(f'The embedding dimension of the all-MiniLM-L6-v2 model is {embedding_dim}.')\n    print(f\"Max sequence lenght of the {model_name} model is {max_seq_len}.\")\n    \n    # Initialize a FAISS index (for CPU)\n    cpu_index = faiss.IndexFlatL2(embedding_dim)\n\n    # Initialize GPU resources for FAISS\n    gpu_index = faiss.index_cpu_to_all_gpus(  # build the index\n        cpu_index\n    )\n    \n    # Loading in the Wikipedia dataset\n    wiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split='train[:5000]')\n    print(f'Lenght of the Wikipedia dataset is {len(wiki_dataset)} articles.')\n    \n    # Process the articles into a list of 256 word strings\n    start_time = time.time()\n    processed_articles = process_strings_and_combine_parallel(wiki_dataset['text'], chunk_size=max_seq_len)\n    end_time = time.time()\n    print(f\"Sequential processing of the dataset took {end_time - start_time:.2f} seconds.\")\n        \n        \n    # Start the multi-process pool on all available CUDA devices\n    pool = model.start_multi_process_pool()\n    \n    # Batch processing with tqdm progress bar\n    batch_size = 1024  # Define batch size based on the system's memory capacity\n    total_batches = len(processed_articles) // batch_size + (0 if len(processed_articles) % batch_size == 0 else 1)\n\n    for i in tqdm(range(0, len(processed_articles), batch_size), total=total_batches, desc=\"Processing Batches\"):\n        # Take the next batch of articles\n        batch_texts = processed_articles[i:i + batch_size]\n        # Compute the embeddings using the multi-process pool\n        batch_embeddings = model.encode_multi_process(batch_texts, pool)\n        # Add embeddings to the GPU index\n        gpu_index.add(batch_embeddings)\n        \n        # Memory management\n        del batch_embeddings, batch_texts\n        gc.collect()\n        \n    # Function to search for relevant articles using GPU\n    def search_wiki_articles(question):\n        question_embedding = model.encode_multi_process(question, pool)\n        distances, indices = gpu_index.search(question_embedding, k=3)\n        return [processed_articles[i] for i in indices[0]]\n\n    # State business questions\n    questions = [\n        'What services does KPMG offer to its clients?',\n        'What are the key considerations when assessing internal controls during an audit?',\n        'How do you stay updated on changes in tax laws and regulations affecting clients?',\n        \"What steps do you take to understand a client's business before initiating a consulting project?\",\n        'What due diligence processes are crucial for evaluating the financial health of a potential acquisition?',\n    ]\n    \n    relevant_article_chunks = [search_wiki_articles(question) for question in questions]\n    \n    # Example\n    print(f'Question:\\n{questions[0]}')\n    print(f'Relevant Wikipedia article chunks:\\n{relevant_article_chunks[0]}')\n    \n    # Optional: Stop the processes in the pool\n    model.stop_multi_process_pool(pool)\n    \n    # (Optional) Save the processed dataset as JSON file and the faiss index\n    \"\"\"\n    import json\n    \n    with open(\"processed_wiki_articles.json\", 'w') as file:\n        json.dump(processed_articles, file)\n        \n    faiss.write_index(gpu_index, 'Wikipedia_FlatL2.index')\n    \"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-20T14:48:58.984244Z","iopub.execute_input":"2024-01-20T14:48:58.985072Z","iopub.status.idle":"2024-01-20T14:58:59.052025Z","shell.execute_reply.started":"2024-01-20T14:48:58.985027Z","shell.execute_reply":"2024-01-20T14:58:59.050986Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b274d593f684abf8d269b93f404004c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37b8dceaff314fe1b6184bb55fd6892c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00b5070caa6f41acb5689ddeea2ae2de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"534d39150c2f47b6acbf72398ec344e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aab93d1ba80b4c3dab6e90fbef6612fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"854ca0aceef6469e977b1d2c28742a34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42587bdd4f1c4b688c5f6ed1af7b7dc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77f1d388ad6147a79b313c7145151c38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d820b03ca192417580b6f9e6e6886b4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c08a08a81350436c95a7051a49281c8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d975b07f5a1481eb36c916c5724309e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5878d49759d54295971485ec903ccd5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cca565137a85402f83e1d7f59d7314cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2dc398d61114f938639df33b2c751c6"}},"metadata":{}},{"name":"stdout","text":"The embedding dimension of the all-MiniLM-L6-v2 model is 384.\nMax sequence lenght of the all-MiniLM-L6-v2 model is 256.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/11.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"494aecc9e07b4e36aa53902bd6643c9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/7.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bca699e6b304e63aa74f4b1e4636002"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset wikipedia/20220301.en (download: 19.18 GiB, generated: 18.88 GiB, post-processed: Unknown size, total: 38.07 GiB) to /root/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/15.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b608a137ee1493dad9332df12d10809"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/20.3G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d61d411b3bb486cbdb7d1ac1ac9509d"}},"metadata":{}},{"name":"stdout","text":"Dataset wikipedia downloaded and prepared to /root/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559. Subsequent calls will reuse this data.\nLenght of the Wikipedia dataset is 5000 articles.\nSequential processing of the dataset took 52.18 seconds.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Batches:   0%|          | 0/67 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf65b87fc9cc4b2985acc94cfd10b5cd"}},"metadata":{}},{"name":"stdout","text":"Question:\nWhat services does KPMG offer to its clients?\nRelevant Wikipedia article chunks:\n['city', 'economic conditions on March 5 2011 WZZM resumed the Saturday and Sunday morning newscasts after a twoyear absence airing for two hours from 6 to 8 am on both days They now also offer a onehour 9 am Newscast on Sunday mornings after GMA In late 2009 WZZM became the third television station in West Michigan to begin broadcasting its local newscasts in widescreen standard definition WWMT was the last major station in West Michigan with 43 standard definition newscasts until April 16 2011 when it became the second station in the market to upgrade to full high definition newscasts In June 2010 WZZM rehired Brent Ashcroft who had left the station twelve years prior to become sports director at Fox affiliate WXMI channel 17 On December 3 2011 WZZM became the fourth and final television station in West Michigan to begin broadcasting its local newscasts in high definition WZZMTV focuses its newscasts on the northern half of the market Grand Rapids and Muskegon with a secondary emphasis on Kalamazoo and Battle Creek On September 8 2014 WZZM began airing news at 5 pm WZZM now has local news from 5 to 630 pm In March 2018 the station rebranded from WZZM 13 to 13 On Your Side At the same time the station switched from the older Gannett styled graphics to the new Tegna 2018 graphics package With the exception of My West Michigan this branding was carried over to all newscasts Weather On Target Forecast WZZMs team of meteorologists compares the accuracy of the', 'by Nielsen Media Research 14 The market is served by 11 full power stations including WEWSTV ABC WJW Fox WKYC NBC WOIO CBS WVIZ PBS WUAB The CW WVPXTV Ion WQHSDT Univision WDLITV Court TV WRLM TCT and the independent WBNXTV The Mike Douglas Show a nationally syndicated daytime talk show began in Cleveland in 1961 on KYWTV now WKYC while The Morning Exchange on WEWSTV served as the model for Good Morning America Tim Conway and Ernie Anderson first established themselves in Cleveland while working together at KYWTV and later WJWTV now WJW Anderson both created and performed as the immensely popular Cleveland horror host Ghoulardi on WJWTVs Shock Theater and was later succeeded by the longrunning late night duo Big Chuck and Lil John Radio Cleveland is directly served by 32 AM and FM radio stations 22 of which are licensed to the city Commercial FM music stations are frequently the highestrated stations in the market WAKS contemporary hits WDOK adult contemporary WENZ mainstream urban WGARFM country WHLK adult hits WMJI classic hits WMMS active rockhot talk WNCX classic rock WNWV alternative rock WQAL hot adult contemporary and WZAK urban adult contemporary WKSU public radio functions as the local NPR affiliate and sister station WCLV airs a classical music format College radio stations include WBWC Baldwin Wallace WCSB Cleveland State WJCU John Carroll and WRUWFM Case Western Reserve Newstalk station WTAM serves as the AM flagship for both the Cleveland Cavaliers and Cleveland Guardians Sports oriented stations include WKNR ESPN WARF Fox and WKRKFM']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The maximum length of tokens the we can feed to the tokenizer before truncation is $512$. Therefore it is no use to search for the k=3 kNN of article chunks for a given question using the faiss index.","metadata":{}},{"cell_type":"code","source":"# Load the Q&A pipeline\nqa_model_name = 'distilbert-base-cased-distilled-squad'\nquestion_answerer = pipeline('question-answering', model=qa_model_name, tokenizer=qa_model_name)\nprint(f'The maximum length of tokens the we can feed to the tokenizer before truncation is {question_answerer.tokenizer.model_max_length}.')\n\n# Function to perform question-answering given a question and a list of documents\ndef answer_question(question, article_chunks):\n    # Combine the article chunks into a single string\n    context = ' '.join(article_chunks)\n\n    # Perform question-answering\n    result = question_answerer(question=question, context=context)\n\n    return result['answer']\n\n# Answer the questions\nresults = [answer_question(questions[idx], relevant_article_chunks[idx]) for idx in range(len(questions))]\n\n#for result in results:\n#    print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n    \nfor idx in range(len(questions)):\n    print(f'Question:\\n{questions[idx]}')\n    print(f'Answer:\\n{results[idx]}')\n    print('='*30)","metadata":{"execution":{"iopub.status.busy":"2024-01-20T15:02:17.266543Z","iopub.execute_input":"2024-01-20T15:02:17.267329Z","iopub.status.idle":"2024-01-20T15:02:21.792959Z","shell.execute_reply.started":"2024-01-20T15:02:17.267296Z","shell.execute_reply":"2024-01-20T15:02:21.79202Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The maximum length of tokens the we can feed to the tokenizer before truncation is 512.\nQuestion:\nWhat services does KPMG offer to its clients?\nAnswer:\n11 full power stations\n==============================\nQuestion:\nWhat are the key considerations when assessing internal controls during an audit?\nAnswer:\naccuracy of the by Nielsen Media Research 14\n==============================\nQuestion:\nHow do you stay updated on changes in tax laws and regulations affecting clients?\nAnswer:\nmedian of a population or thresholding parameter\n==============================\nQuestion:\nWhat steps do you take to understand a client's business before initiating a consulting project?\nAnswer:\nlongrunning late night duo Big Chuck and Lil John Radio Cleveland\n==============================\nQuestion:\nWhat due diligence processes are crucial for evaluating the financial health of a potential acquisition?\nAnswer:\nNielsen Media Research 14\n==============================\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluation of the pipeline\n\nFor evaluation a Q&A pipeline, one an use the [Official Evaluation Script][1] of the [SQuAD v2.0][2] dataset. I inclued a part of the script that I can evaluate the Exact and the F1 score of a pipeline on this dataset.\n\n[1]: https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n[2]: https://rajpurkar.github.io/SQuAD-explorer/","metadata":{}},{"cell_type":"code","source":"\"\"\"Official evaluation script for SQuAD version 2.0.\n\nIn addition to basic functionality, we also compute additional statistics and\nplot precision-recall curves if an additional na_prob.json file is provided.\nThis file is expected to map question ID's to the model's predicted probability\nthat a question is unanswerable.\n\"\"\"\nimport argparse\nimport collections\nimport json\nimport numpy as np\nimport os\nimport re\nimport string\nimport sys\n\nOPTS = None\n\ndef parse_args():\n    parser = argparse.ArgumentParser('Official evaluation script for SQuAD version 2.0.')\n    parser.add_argument('data_file', metavar='data.json', help='Input data JSON file.')\n    parser.add_argument('pred_file', metavar='pred.json', help='Model predictions.')\n    parser.add_argument('--out-file', '-o', metavar='eval.json',\n                        help='Write accuracy metrics to file (default is stdout).')\n    parser.add_argument('--na-prob-file', '-n', metavar='na_prob.json',\n                        help='Model estimates of probability of no answer.')\n    parser.add_argument('--na-prob-thresh', '-t', type=float, default=1.0,\n                        help='Predict \"\" if no-answer probability exceeds this (default = 1.0).')\n    parser.add_argument('--out-image-dir', '-p', metavar='out_images', default=None,\n                        help='Save precision-recall curves to directory.')\n    parser.add_argument('--verbose', '-v', action='store_true')\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n    return parser.parse_args()\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    def remove_articles(text):\n        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n        return re.sub(regex, ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1","metadata":{},"execution_count":null,"outputs":[]}]}