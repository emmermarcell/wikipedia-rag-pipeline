{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84740,"sourceType":"datasetVersion","datasetId":46601},{"sourceId":7470313,"sourceType":"datasetVersion","datasetId":4348765}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/emmermarcell/rag-pipeline-on-the-wikipedia-dataset?scriptVersionId=163356554\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Implementing a RAG pipeline on the Wikipedia dataset\n\nThis notebook shows a creation of a Q&A model on the entire Wikipedia dataset. The original dataset is from [Hugging Face][1], the preprocessing can be found in my [Create a Wikipedia corpus][2] notebook where the articles get broken down into sentences and are stored in .txt.gz files. The output of the notebook is stored in the [Wikipedia Corpus (2023-03-01)][3] Dataset.\n\nThe embedding of the Wikipedia sentences are done with the [`all-MiniLM-L6-v2`][4] model from the [Sentence-Transformers][5] library.  The strings are embedded into a $384$ dimensional vector space where a similarity search is performed by the `faiss.IndexFlatL2` index based on their Euclidean (L2) distance.\n\nThe notebook runs on 2xT4 GPUs that Kaggle provides.\nA great resource for training faiss on multiple GPUs can be found on the [faiss github site][6]. Furthermore, for computing embeddings on multiple GPUs I reference the [Sentence-Transformers github site][7].\n\nAfter the embedding and ranking of athe article chunks, I employ the [`distilbert-base-cased-distilled-squad`][8] Q&A pipeline, a fine-tuned version of the [`DistilBERT-base-cased`][9] model using (a second step of) knowledge distillation on the [`SQuAD v1.1`][10] dataset.\n\nI used the following articles as a starting point for implementing a RAG pipeline:\n\n* [Akriti Upadhyay - Implementing RAG with Langchain and Hugging Face][11]\n\n* [Vladimir Blagojevic - Ask Wikipedia ELI5-like Questions Using Long-Form Question Answering on Haystack][12]\n\n* [Steven van de Graaf - Pre-processing a Wikipedia dump for NLP model training — a write-up][13]\n\n* [Peggy Chang - IVFPQ + HNSW for Billion-scale Similarity Search][14]\n\n\n[1]: https://huggingface.co/datasets/wikipedia\n[2]: https://www.kaggle.com/code/emmermarcell/create-a-wikipedia-corpus\n[3]: https://www.kaggle.com/datasets/emmermarcell/wikipedia-corpus-2023-03-01/versions/1\n[4]: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n[5]: https://www.sbert.net/\n[6]: https://github.com/facebookresearch/faiss/blob/main/tutorial/python/5-Multiple-GPUs.py\n[7]: https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/computing-embeddings/computing_embeddings_multi_gpu.py\n[8]: https://huggingface.co/distilbert-base-cased-distilled-squad\n[9]: https://huggingface.co/distilbert-base-cased\n[10]: https://huggingface.co/datasets/squad\n[11]: https://medium.com/international-school-of-ai-data-science/implementing-rag-with-langchain-and-hugging-face-28e3ea66c5f7\n[12]: https://towardsdatascience.com/ask-wikipedia-eli5-like-questions-using-long-form-question-answering-on-haystack-32cf1ca6c00e\n[13]: https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67\n[14]: https://towardsdatascience.com/ivfpq-hnsw-for-billion-scale-similarity-search-89ff2f89d90e","metadata":{}},{"cell_type":"code","source":"!pip install sentence_transformers\n!pip install faiss-gpu","metadata":{"execution":{"iopub.status.busy":"2024-02-18T22:19:41.616206Z","iopub.execute_input":"2024-02-18T22:19:41.616647Z","iopub.status.idle":"2024-02-18T22:20:13.966426Z","shell.execute_reply.started":"2024-02-18T22:19:41.616598Z","shell.execute_reply":"2024-02-18T22:20:13.965323Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sentence_transformers\n  Obtaining dependency information for sentence_transformers from https://files.pythonhosted.org/packages/06/97/57afa3d05801b6b9305f96a7ce5995e12c1d2ba25ce66747de107816b0b5/sentence_transformers-2.3.1-py3-none-any.whl.metadata\n  Downloading sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.36.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.24.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.11.4)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.20.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence_transformers) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-2.3.1\nCollecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import gc    # Garbage collector\nimport logging\nfrom tqdm.auto import tqdm\n\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import pipeline\nfrom sentence_transformers import SentenceTransformer, LoggingHandler\nimport faiss\n\n\nlogging.basicConfig(\n    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, handlers=[LoggingHandler()]\n)\n\n# Ensure you have a GPU available\nngpus = faiss.get_num_gpus()\nprint(\"number of GPUs:\", ngpus)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T22:20:13.968619Z","iopub.execute_input":"2024-02-18T22:20:13.968963Z","iopub.status.idle":"2024-02-18T22:20:44.666204Z","shell.execute_reply.started":"2024-02-18T22:20:13.968936Z","shell.execute_reply":"2024-02-18T22:20:44.665256Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"number of GPUs: 2\n","output_type":"stream"}]},{"cell_type":"code","source":"# Important, you need to shield your code with if __name__. Otherwise, CUDA runs into issues when spawning new processes.\nif __name__ == \"__main__\":\n    # You can use grep patterns for loading in a Hugging Face dataset from multiple files\n    #wiki_corpus_paths = '/kaggle/input/wikipedia-corpus-2023-03-01/wikipedia_processed_*.txt'\n    wiki_corpus_paths = '/kaggle/input/wikipedia-sentences/wikisent2.txt'\n    processed_wiki_dataset = load_dataset(\"text\", data_files={\"train\": wiki_corpus_paths}, split='train')\n    print(f'The Wikipedia corpus contains {len(processed_wiki_dataset)} sentences.')\n    \n    # Define the sentence transformer model\n    model_name = \"all-MiniLM-L6-v2\"\n    model = SentenceTransformer(model_name)\n    embedding_dim = model.get_sentence_embedding_dimension()    # Get the embedding dimension\n    max_seq_len = model.max_seq_length    # Maximum sequence length in words\n    print(f'The embedding dimension of the all-MiniLM-L6-v2 model is {embedding_dim}.')\n    print(f\"Max sequence lenght of the {model_name} model is {max_seq_len}.\")\n    \n    # Initialize a FAISS index (for CPU)\n    # It performs an exact search using L2 distance topped with with a Hierarchical Navigable Small World structure\n    M = 32            # Number of connections that would be made for each new vertex during HNSW construction.\n    quantizer = faiss.IndexHNSWFlat(\n                        embedding_dim,\n                        M,\n                        faiss.METRIC_INNER_PRODUCT\n                        )    # Coarse quantizer, flat indeces store full vectors\n    # To scale up to large datasetets, compression is used based on a Product Quantizer\n    nlist = 10_000    # Number of cluster\n    nsegment = 16     # Number of subquantizers (segments for product quantization)\n    nbit = 8          # Number of bits to encode each segment.\n    cpu_index = faiss.IndexIVFPQ(\n                        quantizer,\n                        embedding_dim, \n                        nlist, nsegment, nbit,\n                        faiss.METRIC_INNER_PRODUCT\n                        )\n\n    # Provide the available GPU resources to FAISS\n    gpu_index = faiss.index_cpu_to_all_gpus(  # build the index\n        cpu_index\n    )        \n        \n    # Start the multi-process pool on all available CUDA devices\n    pool = model.start_multi_process_pool()\n    \n    # Batch processing with tqdm progress bar\n    # Define batch size based on the system's memory capacity\n    batch_size = 2**19\n    total_batches = len(processed_wiki_dataset['text']) // batch_size + (0 if len(processed_wiki_dataset['text']) % batch_size == 0 else 1)\n    \n    for i in tqdm(range(0, len(processed_wiki_dataset['text']), batch_size), total=total_batches, desc=\"Processing Batches\"):\n        # Take the next batch of articles\n        batch_texts = processed_wiki_dataset['text'][i:i + batch_size]\n        # Compute the embeddings using the multi-process pool\n        batch_embeddings = model.encode_multi_process(\n                                    batch_texts,\n                                    pool,\n                                    normalize_embeddings=True\n                                    )\n        # Train the Faiss index\n        gpu_index.train(batch_embeddings)\n        # Add embeddings to the GPU index\n        gpu_index.add(batch_embeddings)\n        \n        # Memory management\n        del batch_embeddings\n        gc.collect()\n        \n    # Setting the number of partitions to search.\n    gpu_index.nprobe = 1_000\n        \n    # Function to search for relevant articles using GPU\n    def search_wiki_articles(question):\n        question_embedding = model.encode_multi_process(\n                                    question,\n                                    pool,\n                                    normalize_embeddings=True\n                                    )\n        distances, indices = gpu_index.search(question_embedding, k=5)\n        return [processed_wiki_dataset['text'][i] for i in indices[0]]\n\n    # State business questions\n    questions = [\n        'What services does KPMG offer to its clients?',\n        'What are the key considerations when assessing internal controls during an audit?',\n        'How do you stay updated on changes in tax laws and regulations affecting clients?',\n        \"What steps do you take to understand a client's business before initiating a consulting project?\",\n        'What due diligence processes are crucial for evaluating the financial health of a potential acquisition?',\n    ]\n    \n    relevant_article_chunks = [search_wiki_articles(question) for question in questions]\n    \n    # Example\n    print(f'Question:\\n{questions[0]}')\n    print(f'Relevant Wikipedia article chunks:\\n{relevant_article_chunks[0]}')\n    \n    # Optional: Stop the processes in the pool\n    model.stop_multi_process_pool(pool)\n    \n    # (Optional) Save the faiss index\n    # faiss.write_index(cpu_index, 'Wikipedia_HNSWFlat_IndexIVFPQ_IP.index')","metadata":{"execution":{"iopub.status.busy":"2024-02-18T22:20:44.667382Z","iopub.execute_input":"2024-02-18T22:20:44.667974Z","iopub.status.idle":"2024-02-18T23:06:19.140316Z","shell.execute_reply.started":"2024-02-18T22:20:44.667941Z","shell.execute_reply":"2024-02-18T23:06:19.139386Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-d298ec101b1a1667/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af4eb9b8fecc4122b1cd989e34a5a144"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bfe74e8ebb4497a8a5d11177afcf893"}},"metadata":{}},{"name":"stdout","text":"Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-d298ec101b1a1667/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8. Subsequent calls will reuse this data.\nThe Wikipedia corpus contains 7871825 sentences.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baabdf68f4c9446da52dcf733ddfb575"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ed213925f4c400e9bf7586aa115f351"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d51831e3259d48e396a32d87f8491e17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14f39655bbe14e00a596451a8d24e31f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4665ba7891044dccac59974f06b91daf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a5cb81d04634bcab766e103fd225f61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02e1f055454d407e875557b0d5e2fa08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"559577c045e347daabfc4b690e0ff1b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2677f9de330e4d96b3b672329ffe762d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e7984bb60a94930a4fa2ee6092f0957"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a566d7dee19d42f19196e08541a09f86"}},"metadata":{}},{"name":"stdout","text":"The embedding dimension of the all-MiniLM-L6-v2 model is 384.\nMax sequence lenght of the all-MiniLM-L6-v2 model is 256.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Batches:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fb40474ae6b42688cd1c61b6f533685"}},"metadata":{}},{"name":"stdout","text":"Question:\nWhat services does KPMG offer to its clients?\nRelevant Wikipedia article chunks:\n['He received the Wm.', 'W Network (often shortened to W) is a Canadian English language Category A specialty channel, owned by Corus Entertainment.', 'WNWS (1520 AM) is a radio station broadcasting a Soft Adult Contemporary format.', 'WWLW is owned and operated by West Virginia Radio Corporation.', 'WYOU Community Television, Inc (WYOU) is a nonprofit Public, educational, and government access (PEG) cable television station for the Madison, Wisconsin area.']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The maximum length of tokens the we can feed to the `distilbert-base-cased-distilled-squad` tokenizer before truncation is 512. Therefore it is more than enough to search for the k=5 kNN of article chunks for a given question using the faiss index.","metadata":{}},{"cell_type":"code","source":"# Load the Q&A pipeline\nqa_model_name = 'distilbert-base-cased-distilled-squad'\nquestion_answerer = pipeline('question-answering', model=qa_model_name, tokenizer=qa_model_name)\nprint(f'The maximum length of tokens the we can feed to the tokenizer before truncation is {question_answerer.tokenizer.model_max_length}.')\n\n# Function to perform question-answering given a question and a list of documents\ndef answer_question(question, article_chunks):\n    # Combine the article chunks into a single string\n    context = ' '.join(article_chunks)\n\n    # Perform question-answering\n    result = question_answerer(question=question, context=context)\n\n    return result\n\n# Answer the questions\nresults = [answer_question(questions[idx], relevant_article_chunks[idx]) for idx in range(len(questions))]\n\n#for result in results:\n#    print(f\"Answer: '{results[idx]['answer']}', score: {round(results[idx]['score'], 4)}, start: {results[idx]['start']}, end: {results[idx]['end']}\")\n    \nfor idx in range(len(questions)):\n    print(f'Question:\\n{questions[idx]}')\n    print(f\"Answer:\\n'{results[idx]['answer']}',\\nscore:{round(results[idx]['score'], 4)}, start:{results[idx]['start']}, end:{results[idx]['end']}\")\n    print('='*30)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T23:06:19.142328Z","iopub.execute_input":"2024-02-18T23:06:19.142649Z","iopub.status.idle":"2024-02-18T23:06:24.477076Z","shell.execute_reply.started":"2024-02-18T23:06:19.142602Z","shell.execute_reply":"2024-02-18T23:06:24.475932Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0907b112f75049f38858046bfccb8858"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a162c6edba0c43779b2eaf6c1638103f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fd174359bc64a6091f912ea84afc437"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77ed7fdc83de42baaf1d1b93c5dd9996"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"264d1bd43b384740a5cc0d424896ec69"}},"metadata":{}},{"name":"stdout","text":"The maximum length of tokens the we can feed to the tokenizer before truncation is 512.\nQuestion:\nWhat services does KPMG offer to its clients?\nAnswer:\n'Public, educational, and government access',\nscore:0.6501, start:341, end:383\n==============================\nQuestion:\nWhat are the key considerations when assessing internal controls during an audit?\nAnswer:\n'Public, educational, and government access',\nscore:0.1604, start:341, end:383\n==============================\nQuestion:\nHow do you stay updated on changes in tax laws and regulations affecting clients?\nAnswer:\n'high pressure processing',\nscore:0.0326, start:167, end:191\n==============================\nQuestion:\nWhat steps do you take to understand a client's business before initiating a consulting project?\nAnswer:\n'Public, educational, and government access',\nscore:0.1712, start:341, end:383\n==============================\nQuestion:\nWhat due diligence processes are crucial for evaluating the financial health of a potential acquisition?\nAnswer:\n'Public, educational, and government access',\nscore:0.0344, start:341, end:383\n==============================\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluation of the pipeline\n\nFor evaluation a Q&A pipeline, one an use the [Official Evaluation Script][1] of the [SQuAD v2.0][2] dataset. I inclued a part of the script that I can evaluate the Exact and the F1 score of a pipeline on this dataset.\n\n[1]: https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n[2]: https://rajpurkar.github.io/SQuAD-explorer/","metadata":{}},{"cell_type":"code","source":"\"\"\"Official evaluation script for SQuAD version 2.0.\n\nIn addition to basic functionality, we also compute additional statistics and\nplot precision-recall curves if an additional na_prob.json file is provided.\nThis file is expected to map question ID's to the model's predicted probability\nthat a question is unanswerable.\n\"\"\"\nimport argparse\nimport collections\nimport json\nimport numpy as np\nimport os\nimport re\nimport string\nimport sys\n\nOPTS = None\n\ndef parse_args():\n    parser = argparse.ArgumentParser('Official evaluation script for SQuAD version 2.0.')\n    parser.add_argument('data_file', metavar='data.json', help='Input data JSON file.')\n    parser.add_argument('pred_file', metavar='pred.json', help='Model predictions.')\n    parser.add_argument('--out-file', '-o', metavar='eval.json',\n                        help='Write accuracy metrics to file (default is stdout).')\n    parser.add_argument('--na-prob-file', '-n', metavar='na_prob.json',\n                        help='Model estimates of probability of no answer.')\n    parser.add_argument('--na-prob-thresh', '-t', type=float, default=1.0,\n                        help='Predict \"\" if no-answer probability exceeds this (default = 1.0).')\n    parser.add_argument('--out-image-dir', '-p', metavar='out_images', default=None,\n                        help='Save precision-recall curves to directory.')\n    parser.add_argument('--verbose', '-v', action='store_true')\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n    return parser.parse_args()\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    def remove_articles(text):\n        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n        return re.sub(regex, ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1","metadata":{"execution":{"iopub.status.busy":"2024-02-18T23:06:24.479039Z","iopub.execute_input":"2024-02-18T23:06:24.479554Z","iopub.status.idle":"2024-02-18T23:06:24.499427Z","shell.execute_reply.started":"2024-02-18T23:06:24.47951Z","shell.execute_reply":"2024-02-18T23:06:24.498111Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Load in the squad dataset\nsquad_dataset = dataset = load_dataset('squad', split='train')\n\n# Example use\nquestion = squad_dataset['question'][0]\ncontext = squad_dataset['context'][0]\nanswer = squad_dataset['answers'][0]\n\n# Pedict the answer using the distilbert Q&A pipeline\nresult = question_answerer(question=question, context=context)\n\n# Compute the evluation scores\nexact_score = compute_exact(a_gold=answer['text'][0], a_pred=result['answer'])\nf1_score = compute_f1(a_gold=answer['text'][0], a_pred=result['answer'])\n\n\nprint(f'Question:\\n{question}')\nprint(f\"Predited Answer:\\n{result['answer']}\")\nprint(f\"Proper answer accoding to SQuAD:\\n{answer['text'][0]}\")\nprint(f'F1: {f1_score},\\tE: {exact_score}')","metadata":{"execution":{"iopub.status.busy":"2024-02-18T23:06:24.501385Z","iopub.execute_input":"2024-02-18T23:06:24.501871Z","iopub.status.idle":"2024-02-18T23:06:43.036128Z","shell.execute_reply.started":"2024-02-18T23:06:24.501827Z","shell.execute_reply":"2024-02-18T23:06:43.035147Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b539321e565f4ffca5ef479325a61d99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4623b003b78e497ba38b2c536ee1e11c"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f07a4e1b8a4462a9376b04a273b12a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/8.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"128db105b97c43b386e6a8b8d09ffcd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02e169412a874b62a2b2f7bd5ce79e5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42d7f7fac7274f6a85b306ca37ff6234"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\nQuestion:\nTo whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\nPredited Answer:\nSaint Bernadette Soubirous\nProper answer accoding to SQuAD:\nSaint Bernadette Soubirous\nF1: 1.0,\tE: 1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Making the code accessible for end-users\n\nCan be done e.g. with a simple GUI. Some references for how it can be done:\n\n* [Kamalraj M M - Step By Step Guide to Integrate LLM with GUI: Improving Performance Of GUI with LLM][1]\n* [PySimpleGUI][2]\n* Making a simple website from scratch using Flask or Django\n\n[1]: https://www.youtube.com/watch?v=nWi8yM4bCmM\n[2]: https://www.pysimplegui.org/en/latest/#jump-start","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}