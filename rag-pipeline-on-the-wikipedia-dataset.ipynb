{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":160296,"sourceType":"datasetVersion","datasetId":72533}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/emmermarcell/rag-pipeline-on-the-wikipedia-dataset?scriptVersionId=159902283\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Implementing a RAG pipeline on the Wikipedia dataset\n\nSince the dataset is quite large I will make use of the memory mapping between the RAM and the filesystems storage done by the the [Hugging Face Datasets library][1]. Under the hood, it utilizes the Apache Arrow memory format and pyarrow library. Unfortunately the [`wikipedia`][8] dataset is not streamable so I stick to iterating through it.\n\nThe embedding of the Wikipedia articles are done with the [`all-MiniLM-L6-v2`][4] model from the [Sentence-Transformers][5] library.  The strings are embedded into a $384$ dimensional vector space where a similarity search is performed by the `faiss.IndexFlatL2` index based on their Euclidean (L2) distance.\n\nThe notebook runs on 2xT4 GPUs that Kaggle provides.\nA great resource for training faiss on multiple GPUs can be found on the [faiss github site][2]. Furthermore, for computing embeddings on multiple GPUs I reference the [Sentence-Transformers github site][3].\n\nAfter the embedding and ranking of athe article chunks, I employ the [`distilbert-base-cased-distilled-squad`][9] Q&A pipeline, a fine-tuned version of the [`DistilBERT-base-cased`][10] model using (a second step of) knowledge distillation on the [`SQuAD v1.1`][11] dataset.\n\nI used the following articles as a starting point for implementing a RAG pipeline:\n\n* [Akriti Upadhyay - Implementing RAG with Langchain and Hugging Face][6]\n\n* [Vladimir Blagojevic - Ask Wikipedia ELI5-like Questions Using Long-Form Question Answering on Haystack][7]\n\n* [Steven van de Graaf - Pre-processing a Wikipedia dump for NLP model training — a write-up][12]\n\n[1]: https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt\n[2]: https://github.com/facebookresearch/faiss/blob/main/tutorial/python/5-Multiple-GPUs.py\n[3]: https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/computing-embeddings/computing_embeddings_multi_gpu.py\n[4]: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n[5]: https://www.sbert.net/\n[6]: https://medium.com/international-school-of-ai-data-science/implementing-rag-with-langchain-and-hugging-face-28e3ea66c5f7q=implementing+rag+with+langchain+and+huggingface&oq=implementing+rag+with+langchain+and+huggingface&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIKCAEQABiABBiiBDIKCAIQABiABBiiBNIBCDc2MzFqMGo3qAIAsAIA&client=ubuntu-chr&sourceid=chrome&ie=UTF-8\n[7]: https://medium.com/international-school-of-ai-data-science/implementing-rag-with-langchain-and-hugging-face-28e3ea66c5f7\n[8]: https://huggingface.co/datasets/wikipedia\n[9]: https://huggingface.co/distilbert-base-cased-distilled-squad\n[10]: https://huggingface.co/distilbert-base-cased\n[11]: https://huggingface.co/datasets/squad\n[12]: https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67","metadata":{}},{"cell_type":"code","source":"!pip install sentence_transformers\n!pip install faiss-gpu\n!pip install blingfire","metadata":{"execution":{"iopub.status.busy":"2024-01-21T18:23:19.687237Z","iopub.execute_input":"2024-01-21T18:23:19.687617Z","iopub.status.idle":"2024-01-21T18:24:12.428963Z","shell.execute_reply.started":"2024-01-21T18:23:19.687585Z","shell.execute_reply":"2024-01-21T18:24:12.427589Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.36.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.15.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.24.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.11.4)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.20.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\nBuilding wheels for collected packages: sentence_transformers\n  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=645e3f5d974abdc965e25c5f0e7f746a0dcdcf34bbfc6516dd25481417cf93db\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built sentence_transformers\nInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-2.2.2\nCollecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\nCollecting blingfire\n  Downloading blingfire-0.1.8-py3-none-any.whl (42.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: blingfire\nSuccessfully installed blingfire-0.1.8\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport glob\nimport gzip\nimport gc    # Garbage collector\nimport logging\nfrom tqdm.auto import tqdm\n\nfrom blingfire import text_to_sentences\n\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import pipeline\nfrom sentence_transformers import SentenceTransformer, LoggingHandler\nimport faiss\n\n\nlogging.basicConfig(\n    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, handlers=[LoggingHandler()]\n)\n\n# Ensure you have a GPU available\nngpus = faiss.get_num_gpus()\nprint(\"number of GPUs:\", ngpus)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T18:24:12.431551Z","iopub.execute_input":"2024-01-21T18:24:12.431882Z","iopub.status.idle":"2024-01-21T18:24:44.309857Z","shell.execute_reply.started":"2024-01-21T18:24:12.431854Z","shell.execute_reply":"2024-01-21T18:24:44.308716Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"number of GPUs: 2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The `all-MiniLM-L6-v2` model can handle a maximum sequence size of 256 words. As Wikipedia articles are often longer, I use the `text_to_sentences` function from`blingfire` to preprocess the data. This function takes a string (representing a Wikipedia article) and breaks up the article by sentences. The sentences are then saved into the `wikipedia_processed_*.txt` files locally line-by-line. After, I can simply utilize the Huggingface datasets library. ","metadata":{}},{"cell_type":"code","source":"def process_wikipedia_dataset(wiki_dataset, output_dir, articles_per_file=1_000_000):\n    file_count = 1\n    article_count = 0\n\n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Open the first output file for writing in compressed format\n    out_f = gzip.open(f'{output_dir}/wikipedia_processed_{file_count}.txt.gz', 'wt', encoding='utf-8')\n\n    # Iterate through each article in the dataset\n    for article in tqdm(wiki_dataset, desc='Processing Articles'):\n        # Process each line of the article into sentences\n        for line in article['text'].split('\\n'):\n            sentences = text_to_sentences(line)\n            out_f.write(sentences + '\\n')\n\n        article_count += 1\n        # Check if it's time to switch to a new file\n        if article_count >= articles_per_file:\n            out_f.close()\n            file_count += 1\n            article_count = 0\n            out_f = gzip.open(f'{output_dir}/wikipedia_processed_{file_count}.txt.gz', 'wt', encoding='utf-8')\n\n    # Close the last file\n    out_f.close()\n\n    # Pattern to match the compressed files\n    processed_file_paths = f\"{output_dir}/wikipedia_processed_*.txt.gz\"\n    # List all compressed files matching the pattern\n    processed_file_list = glob.glob(processed_file_paths)\n    print(f'The processed files are: {processed_file_list}')\n    \n    return processed_file_list","metadata":{"execution":{"iopub.status.busy":"2024-01-21T18:24:44.31253Z","iopub.execute_input":"2024-01-21T18:24:44.313458Z","iopub.status.idle":"2024-01-21T18:24:44.322974Z","shell.execute_reply.started":"2024-01-21T18:24:44.31342Z","shell.execute_reply":"2024-01-21T18:24:44.321764Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Load the Wikipedia dataset\nwiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split='train')\nprint(f'Length of the Wikipedia dataset is {len(wiki_dataset)} articles.')\n\n# Specify the working directory to save the processed files\noutput_dir = '/kaggle/working'\n# Process the Wikipedia dataset and return the path to the processed files\nprocessed_file_list = process_wikipedia_dataset(wiki_dataset, output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T18:24:44.326027Z","iopub.execute_input":"2024-01-21T18:24:44.32646Z","iopub.status.idle":"2024-01-21T20:20:36.836901Z","shell.execute_reply.started":"2024-01-21T18:24:44.326425Z","shell.execute_reply":"2024-01-21T20:20:36.835952Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/11.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2c70a01daae48908d0721cccd6b1619"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/7.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21b05859ee77481aaad4bbeee6419a71"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset wikipedia/20220301.en (download: 19.18 GiB, generated: 18.88 GiB, post-processed: Unknown size, total: 38.07 GiB) to /root/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/15.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b430822faad94ffea15ae53c7b4b3b31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/20.3G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8ea81d22dbb4bc7a61a18488adec5b3"}},"metadata":{}},{"name":"stdout","text":"Dataset wikipedia downloaded and prepared to /root/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559. Subsequent calls will reuse this data.\nLength of the Wikipedia dataset is 6458670 articles.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Articles:   0%|          | 0/6458670 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce234741519d464bb95a8b9933b83a03"}},"metadata":{}},{"name":"stdout","text":"The processed files are: ['/kaggle/working/wikipedia_processed_1.txt.gz', '/kaggle/working/wikipedia_processed_2.txt.gz', '/kaggle/working/wikipedia_processed_5.txt.gz', '/kaggle/working/wikipedia_processed_6.txt.gz', '/kaggle/working/wikipedia_processed_4.txt.gz', '/kaggle/working/wikipedia_processed_3.txt.gz', '/kaggle/working/wikipedia_processed_7.txt.gz']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Important, you need to shield your code with if __name__. Otherwise, CUDA runs into issues when spawning new processes.\nif __name__ == \"__main__\":\n    # Load in the processed articles into a new Hugging Face dataset\n    processed_wiki_dataset = load_dataset(\"text\", data_files={\"train\": processed_file_list})\n    \n    # Define the sentence transformer model\n    model_name = \"all-MiniLM-L6-v2\"\n    model = SentenceTransformer(model_name)\n    embedding_dim = model.get_sentence_embedding_dimension()    # Get the embedding dimension\n    max_seq_len = model.max_seq_length    # Maximum sequence length in words\n    print(f'The embedding dimension of the all-MiniLM-L6-v2 model is {embedding_dim}.')\n    print(f\"Max sequence lenght of the {model_name} model is {max_seq_len}.\")\n    \n    # Initialize a FAISS index (for CPU)\n    cpu_index = faiss.IndexFlatL2(embedding_dim)\n\n    # Initialize GPU resources for FAISS\n    gpu_index = faiss.index_cpu_to_all_gpus(  # build the index\n        cpu_index\n    )        \n        \n    # Start the multi-process pool on all available CUDA devices\n    pool = model.start_multi_process_pool()\n    \n    # Batch processing with tqdm progress bar\n    batch_size = 64    # Define batch size based on the system's memory capacity\n    for sentence in tqdm(processed_wiki_dataset, desc=\"Embedding Sentences, Adding them to Faiss index\"):\n        # Compute the embeddings using the multi-process pool\n        embeddings = model.encode_multi_process(sentence, pool, batch_size=batch_size)\n        # Add embeddings to the GPU index\n        gpu_index.add(embeddings)\n        \n        # Memory management\n        del embeddings\n        gc.collect()\n        \n    # Function to search for relevant articles using GPU\n    def search_wiki_articles(question):\n        question_embedding = model.encode_multi_process(question, pool)\n        distances, indices = gpu_index.search(question_embedding, k=5)\n        return [processed_wiki_dataset['train']['text'][i] for i in indices[0]]\n\n    # State business questions\n    questions = [\n        'What services does KPMG offer to its clients?',\n        'What are the key considerations when assessing internal controls during an audit?',\n        'How do you stay updated on changes in tax laws and regulations affecting clients?',\n        \"What steps do you take to understand a client's business before initiating a consulting project?\",\n        'What due diligence processes are crucial for evaluating the financial health of a potential acquisition?',\n    ]\n    \n    relevant_article_chunks = [search_wiki_articles(question) for question in questions]\n    \n    # Example\n    print(f'Question:\\n{questions[0]}')\n    print(f'Relevant Wikipedia article chunks:\\n{relevant_article_chunks[0]}')\n    \n    # Optional: Stop the processes in the pool\n    model.stop_multi_process_pool(pool)\n    \n    # (Optional) Save the faiss index\n    # faiss.write_index(gpu_index, 'Wikipedia_FlatL2.index')","metadata":{"execution":{"iopub.status.busy":"2024-01-21T20:24:39.190083Z","iopub.execute_input":"2024-01-21T20:24:39.19054Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-58fc4786dbeb9995/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e4d860ef3774aa180dd181aac0a4d41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"990f837a70e64abbb7701c44f4e923d4"}},"metadata":{}},{"name":"stdout","text":"Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-58fc4786dbeb9995/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a80fe2938e74b4e9fffb7ea7365206e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"460319b1847c4b5caa158cf99f69e050"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97c24b1c5f0946ea84af2d67552892d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3aba500735e04e0c84e70c2fff845da7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6b586a8b99642f095229cf31203b206"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ed7ad636b2741a180a1c55055424304"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70ec2634b9414550a8afa9b1c3fe95e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7af68ae5c494a7ca4225f6efc565a04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"074176b373f2417da3b37b737b01624b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2233c72634124687bfea82992e12ab90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68dd22a139e2423199e35880ebeca809"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d7e0f6aff8e41c9b3ff7b6b753fadc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17b52cd08dd1426cae287c82418c49ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0960f5af7a674cc9b6ed4c3356f6baac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f8e3e5084bb4d0a9fa4bcd37d7ef6bd"}},"metadata":{}},{"name":"stdout","text":"The embedding dimension of the all-MiniLM-L6-v2 model is 384.\nMax sequence lenght of the all-MiniLM-L6-v2 model is 256.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Embedding Sentences, Adding them to Faiss index:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6207592519da4e74a9e902fbbf3f180a"}},"metadata":{}}]},{"cell_type":"code","source":"processed_wiki_dataset['train']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The maximum length of tokens the we can feed to the `distilbert-base-cased-distilled-squad` tokenizer before truncation is 512. Therefore it is more than enough to search for the k=5 kNN of article chunks for a given question using the faiss index.","metadata":{}},{"cell_type":"code","source":"# Load the Q&A pipeline\nqa_model_name = 'distilbert-base-cased-distilled-squad'\nquestion_answerer = pipeline('question-answering', model=qa_model_name, tokenizer=qa_model_name)\nprint(f'The maximum length of tokens the we can feed to the tokenizer before truncation is {question_answerer.tokenizer.model_max_length}.')\n\n# Function to perform question-answering given a question and a list of documents\ndef answer_question(question, article_chunks):\n    # Combine the article chunks into a single string\n    context = ' '.join(article_chunks)\n\n    # Perform question-answering\n    result = question_answerer(question=question, context=context)\n\n    return result\n\n# Answer the questions\nresults = [answer_question(questions[idx], relevant_article_chunks[idx]) for idx in range(len(questions))]\n\n#for result in results:\n#    print(f\"Answer: '{results[idx]['answer']}', score: {round(results[idx]['score'], 4)}, start: {results[idx]['start']}, end: {results[idx]['end']}\")\n    \nfor idx in range(len(questions)):\n    print(f'Question:\\n{questions[idx]}')\n    print(f\"Answer:\\n'{results[idx]['answer']}',\\nscore:{round(results[idx]['score'], 4)}, start:{results[idx]['start']}, end:{results[idx]['end']}\")\n    print('='*30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation of the pipeline\n\nFor evaluation a Q&A pipeline, one an use the [Official Evaluation Script][1] of the [SQuAD v2.0][2] dataset. I inclued a part of the script that I can evaluate the Exact and the F1 score of a pipeline on this dataset.\n\n[1]: https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n[2]: https://rajpurkar.github.io/SQuAD-explorer/","metadata":{}},{"cell_type":"code","source":"\"\"\"Official evaluation script for SQuAD version 2.0.\n\nIn addition to basic functionality, we also compute additional statistics and\nplot precision-recall curves if an additional na_prob.json file is provided.\nThis file is expected to map question ID's to the model's predicted probability\nthat a question is unanswerable.\n\"\"\"\nimport argparse\nimport collections\nimport json\nimport numpy as np\nimport os\nimport re\nimport string\nimport sys\n\nOPTS = None\n\ndef parse_args():\n    parser = argparse.ArgumentParser('Official evaluation script for SQuAD version 2.0.')\n    parser.add_argument('data_file', metavar='data.json', help='Input data JSON file.')\n    parser.add_argument('pred_file', metavar='pred.json', help='Model predictions.')\n    parser.add_argument('--out-file', '-o', metavar='eval.json',\n                        help='Write accuracy metrics to file (default is stdout).')\n    parser.add_argument('--na-prob-file', '-n', metavar='na_prob.json',\n                        help='Model estimates of probability of no answer.')\n    parser.add_argument('--na-prob-thresh', '-t', type=float, default=1.0,\n                        help='Predict \"\" if no-answer probability exceeds this (default = 1.0).')\n    parser.add_argument('--out-image-dir', '-p', metavar='out_images', default=None,\n                        help='Save precision-recall curves to directory.')\n    parser.add_argument('--verbose', '-v', action='store_true')\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n    return parser.parse_args()\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    def remove_articles(text):\n        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n        return re.sub(regex, ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load in the squad dataset\nsquad_dataset = dataset = load_dataset('squad', split='train')\n\n# Example use\nquestion = squad_dataset['question'][0]\ncontext = squad_dataset['context'][0]\nanswer = squad_dataset['answers'][0]\n\n# Pedict the answer using the distilbert Q&A pipeline\nresult = question_answerer(question=question, context=context)\n\n# Compute the evluation scores\nexact_score = compute_exact(a_gold=answer['text'][0], a_pred=result['answer'])\nf1_score = compute_f1(a_gold=answer['text'][0], a_pred=result['answer'])\n\n\nprint(f'Question:\\n{question}')\nprint(f\"Predited Answer:\\n{result['answer']}\")\nprint(f\"Proper answer accoding to SQuAD:\\n{answer['text'][0]}\")\nprint(f'F1: {f1_score},\\tE: {exact_score}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Making the code accessible for end-users\n\nCan be done e.g. with a simple GUI. Some references for how it can be done:\n\n* [Kamalraj M M - Step By Step Guide to Integrate LLM with GUI: Improving Performance Of GUI with LLM][1]\n* [PySimpleGUI][2]\n* Making a simple website from scratch using Flask or Django\n\n[1]: https://www.youtube.com/watch?v=nWi8yM4bCmM\n[2]: https://www.pysimplegui.org/en/latest/#jump-start","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}